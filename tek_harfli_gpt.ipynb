{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.ytimg.com/vi/aircAruvnKk/hqdefault.jpg](https://i.ytimg.com/vi/aircAruvnKk/hqdefault.jpg)\n",
    "\n",
    "[Neural networks (3Blue1Brown) · Course](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=Z4aX4QoYJJe9gOiw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.ytimg.com/vi/l8pRSuU81PU/hqdefault.jpg](https://i.ytimg.com/vi/l8pRSuU81PU/hqdefault.jpg)\n",
    "\n",
    "[Let's reproduce GPT-2 (124M)](https://youtu.be/l8pRSuU81PU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transormer Mimarisi](transformer_mimari.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Attention Blok Şeması](attention_blok_semasi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Matematiksel fonksiyonlar ve sabitler için kullanılır.\n",
    "from dataclasses import dataclass  # Veri sınıflarını basit ve anlaşılır bir şekilde tanımlamak için kullanılır.\n",
    "\n",
    "import torch  # PyTorch kütüphanesi, makine öğrenmesi ve yapay zeka modelleri için kullanılır.\n",
    "import torch.nn as nn  # PyTorch'un sinir ağı (neural network) modüllerini içerir.\n",
    "from torch.nn import functional as F  # PyTorch'un sinir ağı işlevselliklerini içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # tüm başlıklar için key, query, value projeksiyonları, ancak toplu olarak\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # çıktı projeksiyonu\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # düzenleme\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # aslında bir 'bias' değil, daha çok bir maske, ancak OpenAI/HF adlandırmasını takip ediyor\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x, print_info=False):\n",
    "        # x'in şekli [1, 8, 768]\n",
    "        if print_info:\n",
    "            print(\"B, T, C = x.size() = \", x.size())\n",
    "        B, T, C = x.size()  # batch, sıra uzunluğu, gömme boyutu (n_embd)\n",
    "                            # C, transformerdeki kanal sayısıdır, yani kanal sayısı gömme boyutuna eşittir.\n",
    "        # tüm başlıklar için query, key, value hesaplayın ve başlığı ileriye taşıyın\n",
    "        # nh \"başlık sayısı\"dır, hs \"başlık boyutu\"dur ve C (kanal sayısı) = nh * hs'dir\n",
    "        # örneğin GPT-2 (124M) modelinde, n_head=12, hs=64, bu nedenle nh * hs=C=768 Kanal transformerde\n",
    "        qkv = self.c_attn(x)\n",
    "        if print_info:\n",
    "            print(\"qkv.size() = \", qkv.size()) # qkv.size() =  torch.Size([1, 8, 2304])\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        if print_info:\n",
    "            print(\"q.size() = \", q.size()) # q.size() =  torch.Size([1, 8, 768])\n",
    "            print(\"k.size() = \", k.size()) # k.size() =  torch.Size([1, 8, 768])\n",
    "            print(\"v.size() = \", v.size()) # v.size() =  torch.Size([1, 8, 768])\n",
    "        # q, k, v = [B, T, C] -> [B, T, 3, nh, hs] -> [B, nh, T, hs]\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if print_info:\n",
    "            print(\"q.size() = \", q.size()) # q.size() =  torch.Size([1, 12, 8, 64])\n",
    "            print(\"k.size() = \", k.size()) # k.size() =  torch.Size([1, 12, 8, 64])\n",
    "            print(\"v.size() = \", v.size()) # v.size() =  torch.Size([1, 12, 8, 64])\n",
    "        # bu, tüm query ve key'ler için büyük (T,T) matrisini oluşturur\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "                                                                        # (1, 12, 8, 64) x (1, 12, 64, 8) -> (1, 12, 8, 8)\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # matrisi maskelemek için üst üçgeni -inf ile doldur\n",
    "                                                                        # modelin gelecekteki token'lara dikkat etmesini önlemek için\n",
    "        att = F.softmax(att, dim=-1) # (B, nh, T, T) son boyutta softmax\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # tüm başlık çıktıları yan yana yeniden birleştirilir\n",
    "        # çıktı projeksiyonu\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)  # Tam bağlantılı katman, girişten çıkışa lineer dönüşüm\n",
    "        self.gelu    = nn.GELU(approximate='tanh')  # Aktivasyon fonksiyonu olarak GELU (Gaussian Error Linear Unit)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)  # Çıkış projeksiyonu, giriş boyutunu eski haline getirir\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)  # İlk tam bağlantılı katmandan geçiş\n",
    "        x = self.gelu(x)  # GELU aktivasyon fonksiyonunu uygula\n",
    "        x = self.c_proj(x)  # Çıkış projeksiyonu katmanından geçiş\n",
    "        return x  # Çıktıyı döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)  # Birinci Katman Normu\n",
    "        self.attn = CausalSelfAttention(config)  # Nedensel Kendine Dikkat Katmanı\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)  # İkinci Katman Normu\n",
    "        self.mlp = MLP(config)  # Çok Katmanlı Algılayıcı Katmanı\n",
    "\n",
    "    def forward(self, x, print_info=False):\n",
    "        ln_1_x = self.ln_1(x)  # İlk Katman Normunu uygula\n",
    "        if print_info:\n",
    "            print(\"LayerNorm ln_1_x\", ln_1_x.shape, ln_1_x)  # İlk Katman Normunun çıktısını yazdır\n",
    "        x = x + self.attn(ln_1_x)  # Dikkat katmanından gelen çıktıyı girişe ekle\n",
    "        x = x + self.mlp(self.ln_2(x))  # İkinci Katman Normunu uygula ve MLP katmanından gelen çıktıyı ekle\n",
    "        return x  # Çıktıyı döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @dataclass\\nclass GPTConfig:\\n    block_size: int = 1024\\n    vocab_size: int = 50257\\n    n_layer: int = 12\\n    n_head: int = 12\\n    n_embd: int = 768 '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown'daki bazı ufak düzeltmeler yaparak metni yeniden düzenledim:\n",
    "\n",
    "GPT modelinin parametre sayısını hesaplamak için aşağıdaki formüller kullanılır:\n",
    "\n",
    "1. **Embedding katmanı**: \\( vocab\\_size \\times n\\_embd \\)\n",
    "2. **Transformer bloğu**: Her bir Transformer bloğunda toplam parametre sayısı şu şekildedir:\n",
    "   - **LayerNorm**: İki LayerNorm katmanı vardır, her biri \\( 2 \\times n\\_embd \\)\n",
    "   - **Dikkat (Q, K, V)**: \\( 3 \\times (n\\_embd \\times (n\\_embd // n\\_head) + n\\_embd) \\)\n",
    "   - **Dikkat çıktı projeksiyonu**: \\( n\\_embd \\times n\\_embd + n\\_embd \\)\n",
    "   - **MLP (ara katmanlar)**: \\( 2 \\times (n\\_embd \\times 4 \\times n\\_embd + 4 \\times n\\_embd) \\)\n",
    "\n",
    "Bu formülleri kullanarak parametre sayısını hesaplayalım.\n",
    "\n",
    "### Hesaplamalar\n",
    "\n",
    "#### Embedding Katmanı\n",
    "\n",
    "\\[\n",
    "50257 \\times 768 = 38,609,856\n",
    "\\]\n",
    "\n",
    "#### Transformer Bloğu\n",
    "\n",
    "Her bir Transformer bloğunda:\n",
    "\n",
    "- **LayerNorm**: \n",
    "  \\[\n",
    "  2 \\times 768 = 1,536\n",
    "  \\]\n",
    "\n",
    "- **Dikkat (Q, K, V)**:\n",
    "  \\[\n",
    "  3 \\times (768 \\times (768 // 12) + 768) = 3 \\times (768 \\times 64 + 768) = 3 \\times (49,152 + 768) = 3 \\times 49,920 = 149,760\n",
    "  \\]\n",
    "\n",
    "- **Dikkat çıktı projeksiyonu**:\n",
    "  \\[\n",
    "  768 \\times 768 + 768 = 590,592 + 768 = 591,360\n",
    "  \\]\n",
    "\n",
    "- **MLP**:\n",
    "  \\[\n",
    "  2 \\times (768 \\times 4 \\times 768 + 4 \\times 768) = 2 \\times (3,145,728 + 3,072) = 2 \\times 3,148,800 = 6,297,600\n",
    "  \\]\n",
    "\n",
    "Her bir Transformer bloğu için toplam:\n",
    "\\[\n",
    "1,536 + 149,760 + 591,360 + 6,297,600 = 7,040,256\n",
    "\\]\n",
    "\n",
    "#### Toplam Transformer Bloğu Parametreleri\n",
    "\\[\n",
    "12 \\times 7,040,256 = 84,483,072\n",
    "\\]\n",
    "\n",
    "#### Toplam Parametreler\n",
    "\\[\n",
    "38,609,856 (Embedding) + 84,483,072 (Transformer blokları) = 123,092,928\n",
    "\\]\n",
    "\n",
    "Bu hesaplamalar, yaklaşık 124 milyon parametreye oldukça yakındır. Modelin toplam parametre sayısının 124M olduğu, verilen konfigürasyon değerlerine göre bu şekilde hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 64  # Blok boyutu\n",
    "    vocab_size: int = 32  # Kelime hazinesi boyutu\n",
    "    n_layer: int = 12     # Katman sayısı\n",
    "    n_head: int = 12      # Başlık sayısı\n",
    "    n_embd: int = 144     # Gömme boyutu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu konfigürasyon değerlerine göre modelin parametre sayısını yeniden hesaplayalım.\n",
    "\n",
    "### Hesaplamalar\n",
    "\n",
    "#### Embedding Katmanı\n",
    "\n",
    "\\[\n",
    "32 \\times 144 = 4,608\n",
    "\\]\n",
    "\n",
    "#### Transformer Bloğu\n",
    "\n",
    "Her bir Transformer bloğunda:\n",
    "\n",
    "- **LayerNorm**: \n",
    "  \\[\n",
    "  2 \\times 144 = 288\n",
    "  \\]\n",
    "\n",
    "- **Dikkat (Q, K, V)**:\n",
    "  \\[\n",
    "  3 \\times (144 \\times (144 // 12) + 144) = 3 \\times (144 \\times 12 + 144) = 3 \\times (1,728 + 144) = 3 \\times 1,872 = 5,616\n",
    "  \\]\n",
    "\n",
    "- **Dikkat çıktı projeksiyonu**:\n",
    "  \\[\n",
    "  144 \\times 144 + 144 = 20,736 + 144 = 20,880\n",
    "  \\]\n",
    "\n",
    "- **MLP**:\n",
    "  \\[\n",
    "  2 \\times (144 \\times 4 \\times 144 + 4 \\times 144) = 2 \\times (82,944 + 576) = 2 \\times 83,520 = 167,040\n",
    "  \\]\n",
    "\n",
    "Her bir Transformer bloğu için toplam:\n",
    "\\[\n",
    "288 + 5,616 + 20,880 + 167,040 = 193,824\n",
    "\\]\n",
    "\n",
    "#### Toplam Transformer Bloğu Parametreleri\n",
    "\\[\n",
    "12 \\times 193,824 = 2,325,888\n",
    "\\]\n",
    "\n",
    "#### Toplam Parametreler\n",
    "\\[\n",
    "4,608 (Embedding) + 2,325,888 (Transformer blokları) = 2,330,496\n",
    "\\]\n",
    "\n",
    "Bu hesaplamalar, yaklaşık 2.33 milyon parametreye oldukça yakındır. Modelin toplam parametre sayısı bu konfigürasyon değerlerine göre bu şekilde hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def print0(*args, **kwargs):\n",
    "    # sadece master işlemden yazdıran değiştirilmiş print fonksiyonu\n",
    "    # eğer bu dağıtılmış bir çalışma değilse, sadece normal print işlevi gibi çalışır\n",
    "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "        print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # wte: kelime token gömmeleri\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # wpe: pozisyon gömmeleri\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # h: transformer blokları\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # ln_f: çıktıdan önce son layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # lm_head: dil modelleme başlığı\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # bu kısmı başlatma, ağırlıkları bağlayacağız\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # tüm ağırlıkları başlat, çok dikkatli olmak için bir torch rng nesnesi kullan\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # GPT-2 makalesine göre rezidüel projeksiyonlara özel ölçeklenmiş başlatma uygula\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02 / math.sqrt(2 * self.config.n_layer)\n",
    "            # lm_head'i başlatmaktan kaçınmak istiyoruz, çünkü wte ile paylaşılan parametreler\n",
    "            # ve wte Embedding başlatması sırasında zaten başlatıldı\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Bu uzunluktaki bir diziyi ilerletemezsiniz: {t}, blok boyutu yalnızca {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # şekil (t)\n",
    "\n",
    "        # GPT modelini ileri geçir\n",
    "        tok_emb = self.transformer.wte(idx) # token gömmeleri şekli (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # pozisyon gömmeleri şekli (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # eğer istenen hedefler verilmişse, kaybı da hesapla\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # çıkarım zamanında mini optimizasyon: lm_head'i sadece son pozisyonda ileri geçir\n",
    "            logits = self.lm_head(x[:, [-1], :]) # zaman boyutunu korumak için liste [-1] kullanılıyor\n",
    "            loss = None\n",
    "\n",
    "        # performans nedenlerinden dolayı, eğer gerekli değilse logits'i döndürmemek uygun olur\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
    "        # aday parametrelerle başla\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # grad gerektirmeyenleri filtrele\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # optimizasyon grupları oluştur. 2D olan tüm parametreler weight decay'e tabi tutulur, diğerleri tutulmaz.\n",
    "        # yani, matmul'lerdeki ve gömmelerdeki tüm ağırlık tensörleri decay edilir, tüm bias'lar ve layernorm'lar edilmez.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print0(f\"decay edilen parametre tensörlerinin sayısı: {len(decay_params)}, {num_decay_params:,} parametre\")\n",
    "        print0(f\"decay edilmeyen parametre tensörlerinin sayısı: {len(nodecay_params)}, {num_nodecay_params:,} parametre\")\n",
    "        # AdamW optimizasyonu oluştur ve mümkünse birleşik versiyonu kullan\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        print0(f\"birleşik AdamW kullanılıyor: {use_fused}\")\n",
    "        if zero_stage == 1:\n",
    "            print0(\"ZeroRedundancyOptimizer kullanılıyor\")\n",
    "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
    "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
    "            optimizer.add_param_group(optim_groups[1])\n",
    "        else:\n",
    "            print0(\"normal AdamW kullanılıyor\")\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        idx (LongTensor şekli (b,t)) dizininin tamamlanmış halini al ve her seferinde tahminleri modele geri besle.\n",
    "        Büyük olasılıkla model.eval() modunda çalışmak isteyeceksiniz.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # dizinin bağlamı çok uzarsa, blok boyutunda kesmemiz gerekir\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # dizideki indekse ait logits'leri almak için modeli ileri geçir\n",
    "            logits, _ = self(idx_cond)\n",
    "            # logits'leri son adımda alın ve istenen sıcaklıkla ölçeklendirin\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # isteğe bağlı olarak logits'leri sadece top k seçenekleriyle kırpın\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # logits'leri (normalize edilmiş) olasılıklara dönüştürmek için softmax uygulayın\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # dağılımdan örnekleme yap\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # örneklenen indeksi devam eden diziye ekle ve devam et\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    \"\"\"\n",
    "    Dosyanın sadece başlık kısmını okur ve başlık verilerini döndürür.\n",
    "    Burada başlık, dosyanın tamamını okuduğumuz ve byte olarak depoladığımız veri olarak ele alınmıştır.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Okunacak dosyanın adı.\n",
    "\n",
    "    Returns:\n",
    "        int: Dosyadaki byte sayısı.\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()  # Dosyayı byte olarak oku\n",
    "        tokens = [x for x in tokens]  # Byte'ları listeye dönüştür\n",
    "    return len(tokens)  # Byte'ların sayısını döndür\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    \"\"\"\n",
    "    Dosyayı okur ve içindeki tüm verileri byte olarak döndürür.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Okunacak dosyanın adı.\n",
    "\n",
    "    Returns:\n",
    "        list: Dosyadaki byte'ların listesi.\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()  # Dosyayı byte olarak oku\n",
    "        tokens = [x for x in tokens]  # Byte'ları listeye dönüştür\n",
    "    return tokens  # Byte'ların listesini döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        \"\"\"\n",
    "        Dağıtık veri yükleyici başlatılır.\n",
    "        \n",
    "        Args:\n",
    "            filename_pattern (str): Yüklenmesi gereken dosyaların desenini belirten bir dize.\n",
    "            B (int): Batch boyutu.\n",
    "            T (int): Sequence uzunluğu.\n",
    "            process_rank (int): İşlem sırası.\n",
    "            num_processes (int): Toplam işlem sayısı.\n",
    "        \"\"\"\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # Desene uyan dosyaları bul ve sırala\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"Desene uyan hiçbir dosya bulunamadı: {filename_pattern}\"\n",
    "\n",
    "        # Tüm veri parçalarını yükle ve doğrula, toplam token sayısını say\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1, f\"Veri parçacığı {fname} mevcut ayar için çok küçük\"\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print0(f\"Veri Yükleyici: Toplam token sayısı: {ntok_total:,} {len(self.files)} dosyada\")\n",
    "\n",
    "        # Başlatma işlemi\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Veri yükleyiciyi yeniden başlatır.\n",
    "        \"\"\"\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): \n",
    "        \"\"\"\n",
    "        Sonraki veri parçasına ilerler.\n",
    "        \"\"\"\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Bir sonraki batch'i döndürür.\n",
    "        \n",
    "        Returns:\n",
    "            x (torch.Tensor): Giriş tensoru.\n",
    "            y (torch.Tensor): Hedef tensoru.\n",
    "        \"\"\"\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        buf = torch.tensor(buf, dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # Girişler\n",
    "        y = (buf[1:]).view(B, T) # Hedefler\n",
    "        # Mevcut parçacıkta başlangıç işaretçisini ilerlet\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # Eğer bir sonraki batch'i yüklemek sınırları aşarsa, parçacığı ilerlet\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fp16(tensor, file):\n",
    "    \"\"\"\n",
    "    Writes a tensor to a file in float16 format.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Tensor to be written.\n",
    "    - file: File object to write to.\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu().to(torch.float16)  # Convert tensor to float16\n",
    "    b = t.numpy().tobytes()  # Convert tensor to numpy array and then to bytes\n",
    "    file.write(b)  # Write bytes to file\n",
    "\n",
    "def write_fp32(tensor, file):\n",
    "    \"\"\"\n",
    "    Writes a tensor to a file in float32 format.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Tensor to be written.\n",
    "    - file: File object to write to.\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu().to(torch.float32)  # Convert tensor to float32\n",
    "    b = t.numpy().tobytes()  # Convert tensor to numpy array and then to bytes\n",
    "    file.write(b)  # Write bytes to file\n",
    "\n",
    "def write_bf16(tensor, file):\n",
    "    \"\"\"\n",
    "    Writes a tensor to a file in bfloat16 format.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Tensor to be written.\n",
    "    - file: File object to write to.\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu().to(torch.bfloat16)  # Convert tensor to bfloat16\n",
    "    t = t.view(torch.int16)  # Trick: reinterpret as int16 for numpy compatibility\n",
    "    b = t.numpy().tobytes()  # Convert tensor to numpy array and then to bytes\n",
    "    file.write(b)  # Write bytes to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(model_tensors, L, file, dtype):\n",
    "    \"\"\"\n",
    "    GPT-2 modelinin ağırlıklarını bir ikili dosyaya yazar.\n",
    "\n",
    "    Parametreler:\n",
    "    - model_tensors: Model ağırlıklarının tensorlerini içeren sözlük.\n",
    "    - L: Transformer modelindeki katman sayısı.\n",
    "    - file: Tensorlerin yazılacağı dosya nesnesi.\n",
    "    - dtype: Tensorleri dönüştürmek için veri tipi ('float16', 'float32', 'bfloat16').\n",
    "\n",
    "    Notlar:\n",
    "    - 'dtype' parametresine bağlı olarak, uygun yazma fonksiyonunu ('write_fp16', 'write_fp32', 'write_bf16') seçer ve tensorleri dönüştürerek dosyaya yazar.\n",
    "    - GPT-2 modelinin çeşitli bileşenleri için tensorler yazılır; bunlar arasında gömme katmanları, katman normalleştirme ağırlıkları, dikkat ağırlıkları ve sapmaları, MLP ağırlıkları ve sapmaları, ve son katman normalleştirme ağırlıkları bulunur.\n",
    "    \"\"\"\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "\n",
    "    # 'dtype' parametresine göre uygun yazma fonksiyonunu seç\n",
    "    write_fun = write_fp16 if dtype == \"float16\" else write_fp32 if dtype == \"float32\" else write_bf16\n",
    "\n",
    "    # Model tensorlerini dosyaya yaz\n",
    "    write_fun(model_tensors[\"transformer.wte.weight\"], file)  # (V, C)\n",
    "    write_fun(model_tensors[\"transformer.wpe.weight\"], file)  # (T, C)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 3C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 3C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 4C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "\n",
    "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file)  # (C, )\n",
    "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file)  # (C, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pad_vocab(tensor, multiple=128, value=0):\n",
    "    \"\"\"\n",
    "    Tensorin kelime dağarcığı boyutunu en yakın uygun katına kadar doldurur.\n",
    "\n",
    "    Parametreler:\n",
    "    - tensor: (V, C) şeklinde olan giriş tensorü, burada V orijinal kelime dağarcığı boyutunu ve C özellik sayısını belirtir.\n",
    "    - multiple: Kelime dağarcığının doldurulacağı uygun kat.\n",
    "    - value: Dolgu satırlarının doldurulacağı değer.\n",
    "\n",
    "    Dönüş:\n",
    "    - padded: Şekli (Vp, C) olan tensor, burada Vp doldurulmuş kelime dağarcığı boyutunu temsil eder.\n",
    "\n",
    "    Notlar:\n",
    "    - Bu fonksiyon tensorün kelime dağarcığı boyutunu (V) 'multiple' parametresi ile belirtilen en yakın katına kadar doldurur.\n",
    "    - Bu doldurma işlemi, tensor işlemlerini özellikle GPU üzerinde daha verimli hale getirmek için yapılır.\n",
    "    - Algoritmik olarak, bu işlem tensorün anlamını değiştirmez, yalnızca boyutlarını optimize eder.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 2  # Tensorün 2 boyutlu olduğundan emin ol (V, C)\n",
    "    V, C = tensor.shape\n",
    "\n",
    "    # Doldurulmuş kelime dağarcığı boyutunu 'multiple' ile belirtilen en yakın katına yuvarla\n",
    "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
    "\n",
    "    # Gerekirse tensorü doldur\n",
    "    pad_rows = Vp - V\n",
    "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
    "\n",
    "    assert padded.shape == (Vp, C)  # Doldurulmuş tensorün doğru şekilde olduğundan emin ol (Vp, C)\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model, filename, dtype):\n",
    "    \"\"\"\n",
    "    Modeli belirtilen dosyaya yazan fonksiyon.\n",
    "\n",
    "    Parametreler:\n",
    "    - model: Kaydedilecek model.\n",
    "    - filename: Kaydedilecek dosyanın adı.\n",
    "    - dtype: Tensorleri dönüştürmek için veri tipi ('float16', 'float32', 'bfloat16').\n",
    "\n",
    "    Notlar:\n",
    "    - 'dtype' parametresi, geçerli bir veri tipi olmalıdır ('float16', 'float32', 'bfloat16').\n",
    "    - Belirtilen 'dtype' parametresine göre versiyon seçilir ve header oluşturulur.\n",
    "    - Model parametreleri header'ın ardından dosyaya yazılır, önce kelime dağarcığı boyutu 'pad_vocab' fonksiyonuyla uygun bir katına kadar doldurulur.\n",
    "    \"\"\"\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "\n",
    "    # Versiyon numarasını belirle\n",
    "    version = {\n",
    "        \"float16\": 2,  # 2: Tüm tensorler float16, doldurulmuş kelime dağarcığı\n",
    "        \"float32\": 3,  # 3: Tüm tensorler float32, doldurulmuş kelime dağarcığı\n",
    "        \"bfloat16\": 5,  # 5: Tüm tensorler bfloat16, doldurulmuş kelime dağarcığı\n",
    "    }[dtype]\n",
    "\n",
    "    # Header oluştur\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326  # Magic sayı\n",
    "    header[1] = version  # Kontrol noktası versiyonu\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "\n",
    "    # Parametreleri al\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "\n",
    "    # Kelime dağarcığını 128'in katına kadar doldur\n",
    "    wte = params[\"transformer.wte.weight\"]  # (V, C)\n",
    "    wte_padded = pad_vocab(wte)  # (Vp, C)\n",
    "    params[\"transformer.wte.weight\"] = wte_padded  # (Vp, C)\n",
    "    print(f\"{wte.size(0)} boyutundan {wte_padded.size(0)} boyutlu doldurulmuş kelime dağarcığı\")\n",
    "    header[7] = wte_padded.size(0)  # Header'da doldurulmuş kelime dağarcığı boyutunu sakla\n",
    "\n",
    "    # Şimdi dosyaya yaz\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())  # Header'ı yaz\n",
    "        write_tensors(params, model.config.n_layer, file, dtype)  # Parametreleri yaz\n",
    "\n",
    "    print(f\"{filename} dosyası yazıldı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_state(model, x, y, logits, loss, filename, dtype=\"float32\"):\n",
    "    \"\"\"\n",
    "    Model durumunu belirtilen dosyaya yazan fonksiyon.\n",
    "\n",
    "    Parametreler:\n",
    "    - model: Kaydedilecek model.\n",
    "    - x: Modelin girdisi olan tensor.\n",
    "    - y: Modelin hedef çıktısı olan tensor.\n",
    "    - logits: Modelin çıktıları (logitler).\n",
    "    - loss: Modelin kaybı (loss), tek bir float değeri.\n",
    "    - filename: Kaydedilecek dosyanın adı.\n",
    "    - dtype: Tensorleri dönüştürmek için veri tipi ('float32' varsayılan olarak).\n",
    "\n",
    "    Notlar:\n",
    "    - Bu fonksiyon, hata ayıklama için girdi, logitler, kayıp ve parametre gradyanlarını içeren bir durum bilgisi içerir.\n",
    "    - 'dtype' parametresi, geçerli bir veri tipi olmalıdır ('float32', 'float16', 'bfloat16').\n",
    "    - Header oluşturulur ve dosyaya yazılır.\n",
    "    - Girdi (x), hedef (y), logitler, kayıp (loss) ve gradyanlar dosyaya yazılır.\n",
    "    \"\"\"\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327  # Magic sayı\n",
    "    header[1] = 2  # Çalışma durumu versiyonu = 2 (1 -> 2 doldurulmuş kelime dağarcığı değişiklikleri için)\n",
    "    header[2] = x.size(0)  # Batch boyutu B\n",
    "    header[3] = x.size(1)  # Batch'ın zamansal uzunluğu T\n",
    "\n",
    "    # Parametre gradyanlarını al\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "\n",
    "    # Kelime dağarcığını gradyanlarda da uygun katına kadar doldur\n",
    "    wte_grad = grads[\"transformer.wte.weight\"]  # (V, C)\n",
    "    wte_grad_padded = pad_vocab(wte_grad, value=0)  # (Vp, C)\n",
    "    grads[\"transformer.wte.weight\"] = wte_grad_padded  # (Vp, C)\n",
    "    print(f\"Referans gradyanlarda doldurulmuş kelime dağarcığı boyutu {wte_grad.size(0)} boyutundan {wte_grad_padded.size(0)} boyutuna\")\n",
    "\n",
    "    # Dosyaya yaz\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # Header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # Girdi x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes())  # (B, T)\n",
    "        # Hedef y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes())  # (B, T)\n",
    "        # Logitler (model ileri geçişinin sonucu)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # Kayıp (çapraz entropi kaybının tek bir float değeri)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # Gradyanlar\n",
    "        write_tensors(grads, model.config.n_layer, file, dtype)\n",
    "\n",
    "    print(f\"{filename} dosyası yazıldı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenizer(enc, filename):\n",
    "    \"\"\"\n",
    "    Tokenizer'ı belirtilen dosyaya yazan fonksiyon.\n",
    "\n",
    "    Parametreler:\n",
    "    - enc: Encoder (kodlayıcı) nesnesi.\n",
    "    - filename: Kaydedilecek dosyanın adı.\n",
    "\n",
    "    Notlar:\n",
    "    - Bu fonksiyon, tokenizer'ın versiyonunu, token sayısını ve EOT (End-Of-Text) token'ını içeren bir başlık oluşturur ve dosyaya yazar.\n",
    "    - Her bir token için uzunluğunu ve gerçek baytlarını bir dosyaya yazar.\n",
    "    \"\"\"\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328  # Magic sayı\n",
    "    header[1] = 2  # Tokenizer versiyonu = 2 (1 -> 2: EOT token içerir)\n",
    "    header[2] = n  # Token sayısı\n",
    "    header[3] = enc.eot_token  # EOT (End-Of-Text) tokenı\n",
    "\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "\n",
    "        # Her bir token için işlem yap\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])  # Tokenı baytlara çöz\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token uzunluğu 255'i aşıyor: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Uzunluğu 1 byte olarak yaz (unsigned integer)\n",
    "            file.write(b)  # Gerçek baytları yaz\n",
    "\n",
    "    print(f\"{filename} dosyası yazıldı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanılan cihaz: mps (cpu)\n"
     ]
    }
   ],
   "source": [
    "# B, T = 2, 4  # Bu satır şu an yorum olarak işaretlenmiş durumda, kullanılmıyor gibi görünüyor.\n",
    "\n",
    "\"\"\" \n",
    "GPTConfig sınıfıyla ilgili açıklamalar burada yer alabilir. Bu sınıf, GPT modeli için yapılandırma parametrelerini içerir.\n",
    "\"\"\"\n",
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128  # Blok boyutu\n",
    "    vocab_size: int = 64   # Kelime dağarcığı boyutu\n",
    "    n_layer: int = 4       # Katman sayısı\n",
    "    n_head: int = 4        # Kafa (head) sayısı\n",
    "    n_embd: int = 16       # Gömme boyutu\n",
    " \"\"\"\n",
    "B, T = 128, 32  # Batch boyutu ve zaman serisi uzunluğu tanımlanıyor.\n",
    "\n",
    "# Cihaz belirleme\n",
    "device = \"cpu\"  # Varsayılan olarak CPU kullanılıyor\n",
    "if torch.cuda.is_available():  # Eğer CUDA kullanılabilirse,\n",
    "    device = \"cuda\"  # CUDA cihazı kullanılıyor\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # CUDA Memory Pooling Subsystem (MPS) kullanılıyor\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # Cihaz tipi belirleme (cuda veya cpu)\n",
    "print(f\"Kullanılan cihaz: {device} ({device_type})\")  # Kullanılan cihazı ve tipini ekrana yazdırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp_rank = 0  # DDP (DistributedDataParallel) kümesindeki sürecin sırası (rank)\n",
    "ddp_local_rank = 0  # DDP kümesindeki yerel sıra (local rank)\n",
    "zero_stage = 0  # ZeRO-3 optimizasyonunun aşaması\n",
    "ddp_world_size = 1  # DDP kümesindeki toplam süreç sayısı (world size)\n",
    "master_process = True  # Ana süreç (master process) kontrolü\n",
    "seed_offset = 0  # Rastgelelik için başlangıç ofseti\n",
    "total_batch_size = B * T  # Toplam batch boyutu (Batch boyutu * Zaman serisi uzunluğu)\n",
    "tokens_per_fwdbwd = B * T  # İleri ve geri geçişte kullanılan toplam token sayısı\n",
    "tokens_per_fwdbwd  # Bu değişken aynı zamanda tokens_per_fwdbwd olarak tanımlanmıştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam istenen batch boyutu: 4096\n",
      "=> Hesaplanan gradient birikimi adımları: 1\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size ve tokens_per_fwdbwd değişkenlerinin bölümünden kalanın sıfır olduğunu doğrulama\n",
    "assert total_batch_size % tokens_per_fwdbwd == 0\n",
    "\n",
    "# Gradient birikimi adımlarını hesaplamak için total_batch_size ve tokens_per_fwdbwd arasındaki bölümü gerçekleştiriyoruz\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "\n",
    "# Hesaplanan gradient birikimi adımlarını ekrana yazdırma\n",
    "print(f\"Toplam istenen batch boyutu: {total_batch_size}\")\n",
    "print(f\"=> Hesaplanan gradient birikimi adımları: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext  # nullcontext'ı içe aktar\n",
    "\n",
    "# İstenilen dtype ve cihaz tipine göre bir bağlam yöneticisi kurma\n",
    "# 'float16' için ptdtype belirleme\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float8': torch.float8_e4m3fn}['float16']\n",
    "# Cihaz CUDA ise, otomatik karar verme (autocast) ile bağlam oluşturma\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "\n",
    "# Rastgelelik ve çoğaltılabilirlik (reproducibility)\n",
    "torch.manual_seed(42)  # CPU için rastgelelik tohumunu ayarlama\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)  # CUDA kullanılabilirse, CUDA için rastgelelik tohumunu ayarlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(32, 144)\n",
       "    (wpe): Embedding(64, 144)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=144, out_features=432, bias=True)\n",
       "          (c_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=144, out_features=576, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=576, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=144, out_features=32, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yeni bir GPT modeli oluşturuyoruz ve yapılandırma nesnesi ile başlatıyoruz\n",
    "model = GPT(GPTConfig())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # Modeli eğitim moduna geçiriyoruz. Bu, modelin eğitim sırasında gradyanları takip etmesini sağlar.\n",
    "\n",
    "model.to(device)  # Modeli belirtilen cihaza taşıyoruz. 'device' değişkeni, önceden belirlenmiş bir cihaz türünü temsil eder (ör. 'cuda' veya 'cpu').\n",
    "\n",
    "if False:  # Koşul her zaman yanlış olduğu için bu blok çalıştırılmaz.\n",
    "    if hasattr(config, \"coordinate_descent_tuning\"):  # 'config' nesnesinin 'coordinate_descent_tuning' özelliği varsa,\n",
    "        config.coordinate_descent_tuning = True  # Bu özelliği 'True' olarak ayarlıyoruz. Bu ayar, modelin koordinat iniş ayarlamalarını etkinleştirmek için kullanılabilir.\n",
    "    print0(\"compiling the model...\")  # \"model derleniyor...\" mesajını ekrana yazdırıyoruz. 'print0' işlevi muhtemelen 'print' işlevini çağırmak için kullanılan bir işlevdir.\n",
    "    model = torch.compile(model)  # Modeli derlemek için 'torch.compile' işlevini kullanıyoruz. Ancak bu blok şu anda işletilmiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri Yükleyici: Toplam token sayısı: 1,520,928 1 dosyada\n",
      "Veri Yükleyici: Toplam token sayısı: 168,991 1 dosyada\n"
     ]
    }
   ],
   "source": [
    "# Eğitim için dağıtılmış veri yükleyicisi oluşturma\n",
    "train_loader = DistributedDataLoader(\"egitim_jetonlari.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "# \"egitim_jetonlari.bin\" dosyasından verileri yükler. B: batch boyutu, T: zaman serisi uzunluğu, ddp_rank: süreç sırası, ddp_world_size: toplam süreç sayısı.\n",
    "\n",
    "# Doğrulama için dağıtılmış veri yükleyicisi oluşturma\n",
    "val_loader = DistributedDataLoader(\"dogrulama_jetonlari.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "# \"dogrulama_jetonlari.bin\" dosyasından verileri yükler. B: batch boyutu, T: zaman serisi uzunluğu, ddp_rank: süreç sırası, ddp_world_size: toplam süreç sayısı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 32]), torch.Size([128, 32]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "# 'train_loader' üzerinden bir sonraki batch'i alır. Bu işlev, dağıtılmış veri yükleyicisine özgü olabilir ve bir sonraki veri batch'ini döndürür.\n",
    "\n",
    "x, y = x.to(device), y.to(device)\n",
    "# 'x' ve 'y' tensörlerini belirtilen 'device' (cihaz) üzerine taşır. Bu işlem genellikle GPU'lar arasında veri transferi için kullanılır.\n",
    "\n",
    "x.shape, y.shape\n",
    "# 'x' ve 'y' tensörlerinin şekillerini yazdırır. Bu, her bir tensörün boyutlarını ve şeklini kontrol etmek için kullanışlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 boyutundan 128 boyutlu doldurulmuş kelime dağarcığı\n",
      "gpt2_2.33M.bin dosyası yazıldı\n",
      "32 boyutundan 128 boyutlu doldurulmuş kelime dağarcığı\n",
      "gpt2_2.33M_bf16.bin dosyası yazıldı\n",
      "Referans gradyanlarda doldurulmuş kelime dağarcığı boyutu 32 boyutundan 128 boyutuna\n",
      "gpt2_2.33M_debug_state.bin dosyası yazıldı\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x, y)\n",
    "# Modeli, girdi ('x') ve etiket ('y') ile çağırarak çıktı tahminlerini ('logits') ve kaybı ('loss') hesaplar.\n",
    "\n",
    "loss.backward()\n",
    "# Kaybı kullanarak geri yayılım (backpropagation) işlemi yapar, yani gradyanları hesaplar ve modelin parametrelerine uygular.\n",
    "\n",
    "# Model parametrelerini farklı veri tiplerinde ('float32' ve 'bfloat16') kaydetme işlemi\n",
    "model_to_size = {\"tek-harfli-gpt\": \"2.33M\",  \"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
    "model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
    "model_size_str = model_to_size[\"tek-harfli-gpt\"]  # Örneğin \"124M\" veya \"d12\"\n",
    "write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float16\")  # 'float16' veri tipinde modeli kaydeder\n",
    "write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")  # 'bfloat16' veri tipinde modeli kaydeder\n",
    "\n",
    "# Hata ayıklama için x, y, logits, loss ve gradyanları kaydetme işlemi\n",
    "# Referans olarak her zaman 'fp32' (float32) veri tipinde saklanır (?)\n",
    "write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
    "\n",
    "# Optimizasyon için train_loader'ı sıfırlama\n",
    "train_loader.reset()\n",
    "\n",
    "# Model gradyanlarını açıkça sıfırlama işlemi\n",
    "# Çünkü eğitim döngüsünde backward() ve ardından zero_grad() yaparız, bu da ilk eğitim adımında yanlış bir birikme yapabilir\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0001\n",
    "# Ağırlıkların güncellenmesi sırasında uygulanacak L2 düzenlileştirmesi miktarı.\n",
    "\n",
    "learning_rate = 3e-5\n",
    "# Optimizasyon algoritması tarafından kullanılacak başlangıç öğrenme oranı.\n",
    "\n",
    "learning_rate_decay_frac = 0.0001\n",
    "# Her iterasyonda öğrenme oranının azaltılacağı yüzde miktarı.\n",
    "\n",
    "warmup_iters = 0\n",
    "# Eğitim başlamadan önce uygulanacak \"ısınma\" adımı sayısı.\n",
    "\n",
    "num_iterations = 1000\n",
    "# Eğitimin toplam iterasyon sayısı.\n",
    "\n",
    "val_loss_every = 0\n",
    "# Doğrulama kaybının hesaplanacağı iterasyon sayısı. Sıfır ise doğrulama her iterasyonda hesaplanmaz.\n",
    "\n",
    "val_max_steps = 20\n",
    "# Doğrulama işlemi için maksimum adım sayısı.\n",
    "\n",
    "sample_every = 0\n",
    "# Örnekleme sıklığı. Modelin her kaç iterasyonda bir örnek üreteceği.\n",
    "\n",
    "overfit_single_batch = 1\n",
    "# Tek bir batch üzerinde aşırı uyumu kontrol etmek için kullanılan faktör.\n",
    "\n",
    "inference_only = 0\n",
    "# Yalnızca çıkarım modunda çalıştırılıp çalıştırılmayacağı. 1 ise yalnızca çıkarım yapılır, eğitim yapılmaz.\n",
    "\n",
    "grad_clip = 1.0\n",
    "# Gradyanların maksimum normu. Bu değeri aşıldığında gradyanlar belirtilen değere göre kırpılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_model = model # Modeli 'ham_model' adlı bir değişkene kopyalama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay edilen parametre tensörlerinin sayısı: 50, 2,999,808 parametre\n",
      "decay edilmeyen parametre tensörlerinin sayısı: 98, 22,752 parametre\n",
      "birleşik AdamW kullanılıyor: False\n",
      "normal AdamW kullanılıyor\n"
     ]
    }
   ],
   "source": [
    "optimizer = ham_model.configure_optimizers(\n",
    "    weight_decay=weight_decay,  # Ağırlıkların güncellenmesi sırasında uygulanacak L2 düzenlileştirmesi miktarı.\n",
    "    learning_rate=learning_rate,  # Optimizasyon algoritması tarafından kullanılacak öğrenme oranı.\n",
    "    betas=(0.9, 0.95),  # Adam optimizer için beta parametreleri. İlk beta momentum terimi için, ikinci beta RMSProp'un exponential decay terimi için kullanılır.\n",
    "    device_type=device,  # Kullanılan cihaz tipi (\"cuda\" veya \"cpu\").\n",
    "    zero_stage=zero_stage  # Zamanında adımlama (ZeRO) aşamasını belirtir. Bellek kullanımını azaltmak için büyük model eğitimlerinde kullanılır.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    min_lr = learning_rate * learning_rate_decay_frac\n",
    "    # 1) lineer warmup için warmup_iters adımı\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / warmup_iters\n",
    "    # 2) eğer it > num_iterations ise min öğrenme oranını döndür\n",
    "    if it > num_iterations:\n",
    "        return min_lr\n",
    "    # 3) ara değerlerde, min öğrenme oranına kadar kosinüs azalma kullan\n",
    "    decay_ratio = (it - warmup_iters) / (num_iterations - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff 1'den başlar ve 0'a kadar iner\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_list = ['a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'ö', 'p', 'r', 's', 'ş', 't', 'u', 'ü', 'v', 'y', 'z', \n",
    "               ' ', '.', ',']\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Verilen metni harf listesindeki indislerle kodlar.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Kodlanacak metin.\n",
    "\n",
    "    Returns:\n",
    "    - list: Metnin harf listesindeki indislerle kodlanmış hali.\n",
    "    \"\"\"\n",
    "    return [letter_list.index(c) for c in text.lower()]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Verilen indis listesini orijinal metne dönüştürür.\n",
    "\n",
    "    Args:\n",
    "    - ids (list): Kodlanmış metnin indis listesi.\n",
    "\n",
    "    Returns:\n",
    "    - str: Kodlanmış indislerden orijinal metni oluşturan metin.\n",
    "    \"\"\"\n",
    "    return ''.join([letter_list[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0385,  0.0297,  0.0180, -0.0421,  0.0136],\n",
       "        [-0.0195,  0.0192,  0.0324,  0.0290,  0.0054],\n",
       "        [ 0.0195, -0.0203, -0.0108, -0.0088, -0.0063],\n",
       "        [-0.0099,  0.0227, -0.0092,  0.0284,  0.0170],\n",
       "        [-0.0161, -0.0224,  0.0039, -0.0156, -0.0358]], device='mps:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Örnek giriş:  \n",
      "Üretilen çıktı:  bu soruya verilen cevabı değerle\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "def calistir():\n",
    "    # Örnek bir metin oluşturmak için bir örnek\n",
    "    sample_text = \" \"\n",
    "\n",
    "    # Metni harf listesi indisleriyle kodlayıp tensor olarak dönüştürme\n",
    "    sample_tokens = encode(sample_text)\n",
    "    sample_tokens = torch.tensor(sample_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "    # Modelin değerlendirme moduna geçmesi\n",
    "    model.eval()\n",
    "\n",
    "    # Gradyan hesaplama olmadan metin üretme işlemi yapma\n",
    "    with torch.no_grad():\n",
    "        sample_out = model.generate(sample_tokens, max_new_tokens=32, temperature=0.95, top_k=10)\n",
    "\n",
    "    # Üretilen metni ekrana yazdırma\n",
    "    print('---------------')\n",
    "    print(f\"Örnek giriş: {sample_text}\")\n",
    "    print(f\"Üretilen çıktı: {decode(sample_out[0].tolist())}\")\n",
    "    print('---------------')\n",
    "\n",
    "# Fonksiyonu çalıştırma\n",
    "calistir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/1000 | train loss 4.126212 | norm 33.1827 | lr 3.00e-05 | (149.06 ms | 27479 tok/s)\n",
      "step    2/1000 | train loss 3.906522 | norm 30.9736 | lr 3.00e-05 | (111.48 ms | 36741 tok/s)\n",
      "step    3/1000 | train loss 3.666121 | norm 27.4738 | lr 3.00e-05 | (110.60 ms | 37033 tok/s)\n",
      "step    4/1000 | train loss 3.471060 | norm 30.0103 | lr 3.00e-05 | (108.45 ms | 37770 tok/s)\n",
      "step    5/1000 | train loss 3.287433 | norm 24.5010 | lr 3.00e-05 | (111.50 ms | 36737 tok/s)\n",
      "step    6/1000 | train loss 3.106544 | norm 23.6764 | lr 3.00e-05 | (109.24 ms | 37497 tok/s)\n",
      "step    7/1000 | train loss 2.944523 | norm 19.6703 | lr 3.00e-05 | (108.34 ms | 37806 tok/s)\n",
      "step    8/1000 | train loss 2.803646 | norm 15.6169 | lr 3.00e-05 | (108.40 ms | 37785 tok/s)\n",
      "step    9/1000 | train loss 2.682861 | norm 17.9463 | lr 3.00e-05 | (108.15 ms | 37875 tok/s)\n",
      "step   10/1000 | train loss 2.581414 | norm 15.1700 | lr 3.00e-05 | (108.13 ms | 37881 tok/s)\n",
      "step   11/1000 | train loss 2.486064 | norm 12.0744 | lr 3.00e-05 | (108.62 ms | 37710 tok/s)\n",
      "step   12/1000 | train loss 2.395816 | norm 9.6579 | lr 3.00e-05 | (108.22 ms | 37851 tok/s)\n",
      "step   13/1000 | train loss 2.319915 | norm 13.2941 | lr 3.00e-05 | (108.73 ms | 37671 tok/s)\n",
      "step   14/1000 | train loss 2.252601 | norm 11.0239 | lr 3.00e-05 | (107.34 ms | 38160 tok/s)\n",
      "step   15/1000 | train loss 2.196057 | norm 8.8735 | lr 3.00e-05 | (107.47 ms | 38113 tok/s)\n",
      "step   16/1000 | train loss 2.142956 | norm 7.3773 | lr 3.00e-05 | (107.14 ms | 38231 tok/s)\n",
      "step   17/1000 | train loss 2.091128 | norm 9.2165 | lr 3.00e-05 | (109.90 ms | 37271 tok/s)\n",
      "step   18/1000 | train loss 2.040822 | norm 8.0526 | lr 3.00e-05 | (110.47 ms | 37078 tok/s)\n",
      "step   19/1000 | train loss 1.994420 | norm 6.9628 | lr 3.00e-05 | (109.01 ms | 37576 tok/s)\n",
      "step   20/1000 | train loss 1.950200 | norm 6.9902 | lr 3.00e-05 | (111.46 ms | 36748 tok/s)\n",
      "step   21/1000 | train loss 1.904586 | norm 6.2305 | lr 3.00e-05 | (115.73 ms | 35393 tok/s)\n",
      "step   22/1000 | train loss 1.859506 | norm 7.6608 | lr 3.00e-05 | (109.26 ms | 37489 tok/s)\n",
      "step   23/1000 | train loss 1.815714 | norm 6.7687 | lr 3.00e-05 | (108.79 ms | 37649 tok/s)\n",
      "step   24/1000 | train loss 1.773704 | norm 6.8340 | lr 3.00e-05 | (109.49 ms | 37411 tok/s)\n",
      "step   25/1000 | train loss 1.731939 | norm 7.2990 | lr 3.00e-05 | (110.46 ms | 37083 tok/s)\n",
      "step   26/1000 | train loss 1.689765 | norm 6.8457 | lr 3.00e-05 | (110.98 ms | 36907 tok/s)\n",
      "step   27/1000 | train loss 1.649201 | norm 7.2866 | lr 2.99e-05 | (111.98 ms | 36578 tok/s)\n",
      "step   28/1000 | train loss 1.608547 | norm 7.1147 | lr 2.99e-05 | (109.95 ms | 37253 tok/s)\n",
      "step   29/1000 | train loss 1.568172 | norm 6.5494 | lr 2.99e-05 | (108.65 ms | 37699 tok/s)\n",
      "step   30/1000 | train loss 1.528958 | norm 8.2244 | lr 2.99e-05 | (109.78 ms | 37311 tok/s)\n",
      "step   31/1000 | train loss 1.491136 | norm 7.6557 | lr 2.99e-05 | (111.49 ms | 36740 tok/s)\n",
      "step   32/1000 | train loss 1.453150 | norm 7.1016 | lr 2.99e-05 | (109.18 ms | 37515 tok/s)\n",
      "step   33/1000 | train loss 1.418221 | norm 11.3052 | lr 2.99e-05 | (108.96 ms | 37593 tok/s)\n",
      "step   34/1000 | train loss 1.382325 | norm 9.4591 | lr 2.99e-05 | (109.43 ms | 37429 tok/s)\n",
      "step   35/1000 | train loss 1.346175 | norm 6.9758 | lr 2.99e-05 | (110.71 ms | 36996 tok/s)\n",
      "step   36/1000 | train loss 1.313498 | norm 11.2653 | lr 2.99e-05 | (108.67 ms | 37692 tok/s)\n",
      "step   37/1000 | train loss 1.275637 | norm 7.9876 | lr 2.99e-05 | (107.45 ms | 38121 tok/s)\n",
      "step   38/1000 | train loss 1.242744 | norm 10.2407 | lr 2.99e-05 | (113.66 ms | 36037 tok/s)\n",
      "step   39/1000 | train loss 1.207349 | norm 7.4297 | lr 2.99e-05 | (109.70 ms | 37338 tok/s)\n",
      "step   40/1000 | train loss 1.173598 | norm 8.5019 | lr 2.99e-05 | (107.40 ms | 38139 tok/s)\n",
      "step   41/1000 | train loss 1.138920 | norm 7.5114 | lr 2.99e-05 | (106.79 ms | 38354 tok/s)\n",
      "step   42/1000 | train loss 1.106556 | norm 9.7318 | lr 2.99e-05 | (107.45 ms | 38121 tok/s)\n",
      "step   43/1000 | train loss 1.073308 | norm 8.4674 | lr 2.99e-05 | (105.73 ms | 38742 tok/s)\n",
      "step   44/1000 | train loss 1.044048 | norm 11.0032 | lr 2.99e-05 | (107.60 ms | 38067 tok/s)\n",
      "step   45/1000 | train loss 1.013111 | norm 8.4501 | lr 2.99e-05 | (108.19 ms | 37859 tok/s)\n",
      "step   46/1000 | train loss 0.982255 | norm 5.6009 | lr 2.99e-05 | (107.89 ms | 37966 tok/s)\n",
      "step   47/1000 | train loss 0.953223 | norm 10.0745 | lr 2.98e-05 | (108.68 ms | 37690 tok/s)\n",
      "step   48/1000 | train loss 0.924740 | norm 10.8928 | lr 2.98e-05 | (107.79 ms | 37999 tok/s)\n",
      "step   49/1000 | train loss 0.896358 | norm 8.2180 | lr 2.98e-05 | (107.61 ms | 38065 tok/s)\n",
      "step   50/1000 | train loss 0.869114 | norm 7.0957 | lr 2.98e-05 | (108.22 ms | 37850 tok/s)\n",
      "step   51/1000 | train loss 0.840614 | norm 6.7523 | lr 2.98e-05 | (108.31 ms | 37817 tok/s)\n",
      "step   52/1000 | train loss 0.814130 | norm 9.4648 | lr 2.98e-05 | (109.57 ms | 37384 tok/s)\n",
      "step   53/1000 | train loss 0.790569 | norm 11.8242 | lr 2.98e-05 | (111.30 ms | 36800 tok/s)\n",
      "step   54/1000 | train loss 0.764205 | norm 8.1053 | lr 2.98e-05 | (110.14 ms | 37188 tok/s)\n",
      "step   55/1000 | train loss 0.742104 | norm 9.7031 | lr 2.98e-05 | (107.84 ms | 37983 tok/s)\n",
      "step   56/1000 | train loss 0.717805 | norm 7.1264 | lr 2.98e-05 | (107.86 ms | 37975 tok/s)\n",
      "step   57/1000 | train loss 0.695879 | norm 8.8507 | lr 2.98e-05 | (108.69 ms | 37686 tok/s)\n",
      "step   58/1000 | train loss 0.672038 | norm 6.8867 | lr 2.98e-05 | (111.28 ms | 36807 tok/s)\n",
      "step   59/1000 | train loss 0.651311 | norm 9.5194 | lr 2.98e-05 | (109.10 ms | 37542 tok/s)\n",
      "step   60/1000 | train loss 0.632360 | norm 9.7200 | lr 2.97e-05 | (106.84 ms | 38338 tok/s)\n",
      "step   61/1000 | train loss 0.610654 | norm 7.3121 | lr 2.97e-05 | (108.56 ms | 37730 tok/s)\n",
      "step   62/1000 | train loss 0.599485 | norm 15.6979 | lr 2.97e-05 | (109.39 ms | 37445 tok/s)\n",
      "step   63/1000 | train loss 0.575700 | norm 5.6203 | lr 2.97e-05 | (109.15 ms | 37525 tok/s)\n",
      "step   64/1000 | train loss 0.568994 | norm 17.1218 | lr 2.97e-05 | (111.33 ms | 36793 tok/s)\n",
      "step   65/1000 | train loss 0.549559 | norm 10.2019 | lr 2.97e-05 | (109.07 ms | 37554 tok/s)\n",
      "step   66/1000 | train loss 0.535659 | norm 14.0985 | lr 2.97e-05 | (108.98 ms | 37585 tok/s)\n",
      "step   67/1000 | train loss 0.519962 | norm 10.3792 | lr 2.97e-05 | (108.93 ms | 37601 tok/s)\n",
      "step   68/1000 | train loss 0.507301 | norm 12.0021 | lr 2.97e-05 | (107.33 ms | 38163 tok/s)\n",
      "step   69/1000 | train loss 0.494443 | norm 10.5370 | lr 2.97e-05 | (109.30 ms | 37477 tok/s)\n",
      "step   70/1000 | train loss 0.479389 | norm 10.5556 | lr 2.96e-05 | (108.45 ms | 37769 tok/s)\n",
      "step   71/1000 | train loss 0.468316 | norm 10.7993 | lr 2.96e-05 | (108.26 ms | 37835 tok/s)\n",
      "step   72/1000 | train loss 0.454120 | norm 8.4217 | lr 2.96e-05 | (107.97 ms | 37937 tok/s)\n",
      "step   73/1000 | train loss 0.441358 | norm 8.4990 | lr 2.96e-05 | (110.44 ms | 37089 tok/s)\n",
      "step   74/1000 | train loss 0.432977 | norm 10.2650 | lr 2.96e-05 | (109.33 ms | 37465 tok/s)\n",
      "step   75/1000 | train loss 0.418220 | norm 6.4066 | lr 2.96e-05 | (106.82 ms | 38346 tok/s)\n",
      "step   76/1000 | train loss 0.408202 | norm 8.5128 | lr 2.96e-05 | (106.85 ms | 38336 tok/s)\n",
      "step   77/1000 | train loss 0.400468 | norm 9.0104 | lr 2.96e-05 | (107.65 ms | 38048 tok/s)\n",
      "step   78/1000 | train loss 0.386835 | norm 6.1250 | lr 2.96e-05 | (107.41 ms | 38135 tok/s)\n",
      "step   79/1000 | train loss 0.379346 | norm 8.6909 | lr 2.96e-05 | (107.88 ms | 37969 tok/s)\n",
      "step   80/1000 | train loss 0.369262 | norm 7.0567 | lr 2.95e-05 | (108.19 ms | 37859 tok/s)\n",
      "step   81/1000 | train loss 0.360139 | norm 8.2247 | lr 2.95e-05 | (107.81 ms | 37993 tok/s)\n",
      "step   82/1000 | train loss 0.351231 | norm 6.5960 | lr 2.95e-05 | (108.47 ms | 37761 tok/s)\n",
      "step   83/1000 | train loss 0.344382 | norm 9.0648 | lr 2.95e-05 | (107.45 ms | 38120 tok/s)\n",
      "step   84/1000 | train loss 0.335801 | norm 6.6002 | lr 2.95e-05 | (108.66 ms | 37696 tok/s)\n",
      "step   85/1000 | train loss 0.326499 | norm 5.0621 | lr 2.95e-05 | (107.86 ms | 37974 tok/s)\n",
      "step   86/1000 | train loss 0.322037 | norm 8.3876 | lr 2.95e-05 | (108.61 ms | 37712 tok/s)\n",
      "step   87/1000 | train loss 0.313222 | norm 5.1718 | lr 2.95e-05 | (108.21 ms | 37852 tok/s)\n",
      "step   88/1000 | train loss 0.306377 | norm 5.6798 | lr 2.94e-05 | (107.41 ms | 38135 tok/s)\n",
      "step   89/1000 | train loss 0.300281 | norm 5.7005 | lr 2.94e-05 | (107.41 ms | 38134 tok/s)\n",
      "step   90/1000 | train loss 0.293564 | norm 7.0878 | lr 2.94e-05 | (108.36 ms | 37800 tok/s)\n",
      "step   91/1000 | train loss 0.290515 | norm 8.0749 | lr 2.94e-05 | (108.05 ms | 37908 tok/s)\n",
      "step   92/1000 | train loss 0.284026 | norm 6.4524 | lr 2.94e-05 | (109.03 ms | 37567 tok/s)\n",
      "step   93/1000 | train loss 0.280089 | norm 8.4195 | lr 2.94e-05 | (113.51 ms | 36086 tok/s)\n",
      "step   94/1000 | train loss 0.273926 | norm 5.5313 | lr 2.94e-05 | (108.94 ms | 37600 tok/s)\n",
      "step   95/1000 | train loss 0.269886 | norm 7.1539 | lr 2.94e-05 | (110.92 ms | 36929 tok/s)\n",
      "step   96/1000 | train loss 0.264973 | norm 6.4990 | lr 2.93e-05 | (110.21 ms | 37166 tok/s)\n",
      "step   97/1000 | train loss 0.259988 | norm 5.1927 | lr 2.93e-05 | (109.41 ms | 37437 tok/s)\n",
      "step   98/1000 | train loss 0.258669 | norm 8.8746 | lr 2.93e-05 | (109.87 ms | 37279 tok/s)\n",
      "step   99/1000 | train loss 0.253903 | norm 6.2777 | lr 2.93e-05 | (110.93 ms | 36925 tok/s)\n",
      "step  100/1000 | train loss 0.247479 | norm 5.3190 | lr 2.93e-05 | (108.22 ms | 37851 tok/s)\n",
      "step  101/1000 | train loss 0.246609 | norm 8.5345 | lr 2.93e-05 | (108.31 ms | 37818 tok/s)\n",
      "step  102/1000 | train loss 0.240734 | norm 5.7161 | lr 2.93e-05 | (109.26 ms | 37488 tok/s)\n",
      "step  103/1000 | train loss 0.236804 | norm 6.1405 | lr 2.92e-05 | (108.58 ms | 37723 tok/s)\n",
      "step  104/1000 | train loss 0.233229 | norm 5.6823 | lr 2.92e-05 | (108.68 ms | 37687 tok/s)\n",
      "step  105/1000 | train loss 0.229431 | norm 5.6103 | lr 2.92e-05 | (107.50 ms | 38102 tok/s)\n",
      "step  106/1000 | train loss 0.226310 | norm 5.1255 | lr 2.92e-05 | (107.27 ms | 38184 tok/s)\n",
      "step  107/1000 | train loss 0.223780 | norm 5.6594 | lr 2.92e-05 | (107.93 ms | 37950 tok/s)\n",
      "step  108/1000 | train loss 0.222121 | norm 6.9722 | lr 2.92e-05 | (107.02 ms | 38273 tok/s)\n",
      "step  109/1000 | train loss 0.219261 | norm 6.1648 | lr 2.91e-05 | (106.67 ms | 38400 tok/s)\n",
      "step  110/1000 | train loss 0.215121 | norm 4.2787 | lr 2.91e-05 | (106.55 ms | 38442 tok/s)\n",
      "step  111/1000 | train loss 0.212891 | norm 5.3972 | lr 2.91e-05 | (108.02 ms | 37920 tok/s)\n",
      "step  112/1000 | train loss 0.211573 | norm 5.9413 | lr 2.91e-05 | (108.31 ms | 37816 tok/s)\n",
      "step  113/1000 | train loss 0.207651 | norm 4.4512 | lr 2.91e-05 | (108.11 ms | 37888 tok/s)\n",
      "step  114/1000 | train loss 0.207449 | norm 5.9484 | lr 2.91e-05 | (108.65 ms | 37699 tok/s)\n",
      "step  115/1000 | train loss 0.203883 | norm 3.8856 | lr 2.90e-05 | (108.40 ms | 37785 tok/s)\n",
      "step  116/1000 | train loss 0.202304 | norm 4.0250 | lr 2.90e-05 | (108.18 ms | 37862 tok/s)\n",
      "step  117/1000 | train loss 0.200227 | norm 4.6550 | lr 2.90e-05 | (108.69 ms | 37685 tok/s)\n",
      "step  118/1000 | train loss 0.197934 | norm 4.8840 | lr 2.90e-05 | (109.12 ms | 37536 tok/s)\n",
      "step  119/1000 | train loss 0.196262 | norm 4.3050 | lr 2.90e-05 | (110.17 ms | 37180 tok/s)\n",
      "step  120/1000 | train loss 0.194607 | norm 4.9046 | lr 2.90e-05 | (109.91 ms | 37268 tok/s)\n",
      "step  121/1000 | train loss 0.191672 | norm 4.2600 | lr 2.89e-05 | (111.11 ms | 36865 tok/s)\n",
      "step  122/1000 | train loss 0.191472 | norm 6.2639 | lr 2.89e-05 | (111.44 ms | 36756 tok/s)\n",
      "step  123/1000 | train loss 0.187748 | norm 3.3008 | lr 2.89e-05 | (110.57 ms | 37044 tok/s)\n",
      "step  124/1000 | train loss 0.188365 | norm 6.1805 | lr 2.89e-05 | (109.44 ms | 37425 tok/s)\n",
      "step  125/1000 | train loss 0.187540 | norm 5.1915 | lr 2.89e-05 | (107.84 ms | 37983 tok/s)\n",
      "step  126/1000 | train loss 0.185094 | norm 4.2245 | lr 2.89e-05 | (108.64 ms | 37704 tok/s)\n",
      "step  127/1000 | train loss 0.185166 | norm 6.2759 | lr 2.88e-05 | (107.39 ms | 38140 tok/s)\n",
      "step  128/1000 | train loss 0.184008 | norm 4.2648 | lr 2.88e-05 | (108.16 ms | 37869 tok/s)\n",
      "step  129/1000 | train loss 0.182560 | norm 5.0535 | lr 2.88e-05 | (107.46 ms | 38118 tok/s)\n",
      "step  130/1000 | train loss 0.181506 | norm 4.1157 | lr 2.88e-05 | (107.04 ms | 38265 tok/s)\n",
      "step  131/1000 | train loss 0.180956 | norm 6.3646 | lr 2.88e-05 | (110.79 ms | 36972 tok/s)\n",
      "step  132/1000 | train loss 0.180716 | norm 4.2516 | lr 2.87e-05 | (107.98 ms | 37934 tok/s)\n",
      "step  133/1000 | train loss 0.179554 | norm 4.8803 | lr 2.87e-05 | (107.86 ms | 37975 tok/s)\n",
      "step  134/1000 | train loss 0.177977 | norm 4.4241 | lr 2.87e-05 | (109.16 ms | 37524 tok/s)\n",
      "step  135/1000 | train loss 0.176568 | norm 4.8850 | lr 2.87e-05 | (109.04 ms | 37563 tok/s)\n",
      "step  136/1000 | train loss 0.174939 | norm 3.8893 | lr 2.87e-05 | (108.68 ms | 37688 tok/s)\n",
      "step  137/1000 | train loss 0.174089 | norm 3.7188 | lr 2.87e-05 | (107.99 ms | 37929 tok/s)\n",
      "step  138/1000 | train loss 0.172875 | norm 3.5479 | lr 2.86e-05 | (108.86 ms | 37627 tok/s)\n",
      "step  139/1000 | train loss 0.171642 | norm 3.9675 | lr 2.86e-05 | (139.86 ms | 29287 tok/s)\n",
      "step  140/1000 | train loss 0.171206 | norm 4.1303 | lr 2.86e-05 | (106.80 ms | 38350 tok/s)\n",
      "step  141/1000 | train loss 0.169644 | norm 3.5830 | lr 2.86e-05 | (107.49 ms | 38106 tok/s)\n",
      "step  142/1000 | train loss 0.168719 | norm 3.3783 | lr 2.86e-05 | (139.30 ms | 29404 tok/s)\n",
      "step  143/1000 | train loss 0.168227 | norm 3.1900 | lr 2.85e-05 | (108.12 ms | 37885 tok/s)\n",
      "step  144/1000 | train loss 0.167182 | norm 3.3642 | lr 2.85e-05 | (108.10 ms | 37889 tok/s)\n",
      "step  145/1000 | train loss 0.166422 | norm 3.3805 | lr 2.85e-05 | (108.31 ms | 37818 tok/s)\n",
      "step  146/1000 | train loss 0.166336 | norm 4.0801 | lr 2.85e-05 | (110.88 ms | 36942 tok/s)\n",
      "step  147/1000 | train loss 0.165095 | norm 2.9091 | lr 2.84e-05 | (110.91 ms | 36931 tok/s)\n",
      "step  148/1000 | train loss 0.163922 | norm 3.1385 | lr 2.84e-05 | (108.20 ms | 37855 tok/s)\n",
      "step  149/1000 | train loss 0.164660 | norm 4.4594 | lr 2.84e-05 | (111.51 ms | 36734 tok/s)\n",
      "step  150/1000 | train loss 0.162129 | norm 3.7022 | lr 2.84e-05 | (112.19 ms | 36508 tok/s)\n",
      "step  151/1000 | train loss 0.162003 | norm 3.7818 | lr 2.84e-05 | (110.72 ms | 36994 tok/s)\n",
      "step  152/1000 | train loss 0.161005 | norm 2.7627 | lr 2.83e-05 | (109.19 ms | 37513 tok/s)\n",
      "step  153/1000 | train loss 0.160878 | norm 3.3819 | lr 2.83e-05 | (109.83 ms | 37293 tok/s)\n",
      "step  154/1000 | train loss 0.161272 | norm 4.2817 | lr 2.83e-05 | (109.30 ms | 37476 tok/s)\n",
      "step  155/1000 | train loss 0.158342 | norm 2.4253 | lr 2.83e-05 | (110.04 ms | 37224 tok/s)\n",
      "step  156/1000 | train loss 0.160480 | norm 4.6551 | lr 2.83e-05 | (108.28 ms | 37828 tok/s)\n",
      "step  157/1000 | train loss 0.159149 | norm 3.1955 | lr 2.82e-05 | (109.86 ms | 37285 tok/s)\n",
      "step  158/1000 | train loss 0.158160 | norm 3.2253 | lr 2.82e-05 | (110.00 ms | 37237 tok/s)\n",
      "step  159/1000 | train loss 0.158145 | norm 3.8883 | lr 2.82e-05 | (111.91 ms | 36602 tok/s)\n",
      "step  160/1000 | train loss 0.156660 | norm 2.5830 | lr 2.82e-05 | (112.11 ms | 36534 tok/s)\n",
      "step  161/1000 | train loss 0.157206 | norm 3.4408 | lr 2.81e-05 | (111.02 ms | 36894 tok/s)\n",
      "step  162/1000 | train loss 0.156707 | norm 3.3293 | lr 2.81e-05 | (110.39 ms | 37105 tok/s)\n",
      "step  163/1000 | train loss 0.156158 | norm 3.5992 | lr 2.81e-05 | (110.79 ms | 36972 tok/s)\n",
      "step  164/1000 | train loss 0.155713 | norm 3.6686 | lr 2.81e-05 | (110.63 ms | 37024 tok/s)\n",
      "step  165/1000 | train loss 0.154369 | norm 2.7860 | lr 2.81e-05 | (107.69 ms | 38033 tok/s)\n",
      "step  166/1000 | train loss 0.153878 | norm 2.7286 | lr 2.80e-05 | (107.90 ms | 37959 tok/s)\n",
      "step  167/1000 | train loss 0.153745 | norm 3.3258 | lr 2.80e-05 | (109.09 ms | 37547 tok/s)\n",
      "step  168/1000 | train loss 0.153654 | norm 5.1046 | lr 2.80e-05 | (109.54 ms | 37392 tok/s)\n",
      "step  169/1000 | train loss 0.153580 | norm 3.2075 | lr 2.80e-05 | (109.09 ms | 37549 tok/s)\n",
      "step  170/1000 | train loss 0.153358 | norm 3.9671 | lr 2.79e-05 | (113.49 ms | 36090 tok/s)\n",
      "step  171/1000 | train loss 0.151980 | norm 2.8015 | lr 2.79e-05 | (110.32 ms | 37127 tok/s)\n",
      "step  172/1000 | train loss 0.152526 | norm 3.3235 | lr 2.79e-05 | (109.48 ms | 37415 tok/s)\n",
      "step  173/1000 | train loss 0.151832 | norm 4.8428 | lr 2.79e-05 | (112.50 ms | 36408 tok/s)\n",
      "step  174/1000 | train loss 0.152048 | norm 3.1391 | lr 2.78e-05 | (109.75 ms | 37320 tok/s)\n",
      "step  175/1000 | train loss 0.152381 | norm 6.2500 | lr 2.78e-05 | (108.78 ms | 37655 tok/s)\n",
      "step  176/1000 | train loss 0.152840 | norm 4.0149 | lr 2.78e-05 | (109.39 ms | 37444 tok/s)\n",
      "step  177/1000 | train loss 0.150524 | norm 3.5899 | lr 2.78e-05 | (107.79 ms | 37998 tok/s)\n",
      "step  178/1000 | train loss 0.150988 | norm 3.6545 | lr 2.77e-05 | (107.26 ms | 38187 tok/s)\n",
      "step  179/1000 | train loss 0.151073 | norm 3.8006 | lr 2.77e-05 | (109.06 ms | 37557 tok/s)\n",
      "step  180/1000 | train loss 0.149212 | norm 2.8659 | lr 2.77e-05 | (136.96 ms | 29905 tok/s)\n",
      "step  181/1000 | train loss 0.150227 | norm 3.9004 | lr 2.77e-05 | (107.80 ms | 37997 tok/s)\n",
      "step  182/1000 | train loss 0.149108 | norm 2.7864 | lr 2.76e-05 | (108.42 ms | 37780 tok/s)\n",
      "step  183/1000 | train loss 0.148129 | norm 2.7872 | lr 2.76e-05 | (109.87 ms | 37281 tok/s)\n",
      "step  184/1000 | train loss 0.150205 | norm 4.4571 | lr 2.76e-05 | (107.12 ms | 38238 tok/s)\n",
      "step  185/1000 | train loss 0.148345 | norm 2.7359 | lr 2.76e-05 | (109.77 ms | 37313 tok/s)\n",
      "step  186/1000 | train loss 0.148385 | norm 3.9936 | lr 2.75e-05 | (107.97 ms | 37938 tok/s)\n",
      "step  187/1000 | train loss 0.147639 | norm 3.0966 | lr 2.75e-05 | (108.14 ms | 37876 tok/s)\n",
      "step  188/1000 | train loss 0.147352 | norm 3.1413 | lr 2.75e-05 | (107.73 ms | 38019 tok/s)\n",
      "step  189/1000 | train loss 0.146307 | norm 2.5161 | lr 2.75e-05 | (107.35 ms | 38155 tok/s)\n",
      "step  190/1000 | train loss 0.146773 | norm 3.4604 | lr 2.74e-05 | (107.30 ms | 38175 tok/s)\n",
      "step  191/1000 | train loss 0.146071 | norm 2.9695 | lr 2.74e-05 | (106.84 ms | 38337 tok/s)\n",
      "step  192/1000 | train loss 0.145246 | norm 2.5816 | lr 2.74e-05 | (107.22 ms | 38202 tok/s)\n",
      "step  193/1000 | train loss 0.144243 | norm 2.0513 | lr 2.74e-05 | (109.94 ms | 37257 tok/s)\n",
      "step  194/1000 | train loss 0.144920 | norm 2.6474 | lr 2.73e-05 | (112.25 ms | 36492 tok/s)\n",
      "step  195/1000 | train loss 0.143564 | norm 1.8452 | lr 2.73e-05 | (112.11 ms | 36535 tok/s)\n",
      "step  196/1000 | train loss 0.144752 | norm 3.6628 | lr 2.73e-05 | (108.57 ms | 37726 tok/s)\n",
      "step  197/1000 | train loss 0.144619 | norm 2.8352 | lr 2.72e-05 | (110.36 ms | 37114 tok/s)\n",
      "step  198/1000 | train loss 0.143038 | norm 1.9712 | lr 2.72e-05 | (106.16 ms | 38583 tok/s)\n",
      "step  199/1000 | train loss 0.142646 | norm 2.4242 | lr 2.72e-05 | (108.12 ms | 37884 tok/s)\n",
      "step  200/1000 | train loss 0.143219 | norm 2.6190 | lr 2.72e-05 | (106.80 ms | 38352 tok/s)\n",
      "step  201/1000 | train loss 0.141952 | norm 2.2726 | lr 2.71e-05 | (109.07 ms | 37554 tok/s)\n",
      "step  202/1000 | train loss 0.142776 | norm 3.4236 | lr 2.71e-05 | (109.36 ms | 37454 tok/s)\n",
      "step  203/1000 | train loss 0.141895 | norm 2.2644 | lr 2.71e-05 | (109.55 ms | 37389 tok/s)\n",
      "step  204/1000 | train loss 0.141064 | norm 2.4741 | lr 2.71e-05 | (110.01 ms | 37233 tok/s)\n",
      "step  205/1000 | train loss 0.140829 | norm 2.4581 | lr 2.70e-05 | (107.69 ms | 38034 tok/s)\n",
      "step  206/1000 | train loss 0.140265 | norm 2.3103 | lr 2.70e-05 | (109.76 ms | 37319 tok/s)\n",
      "step  207/1000 | train loss 0.140458 | norm 2.6442 | lr 2.70e-05 | (109.08 ms | 37549 tok/s)\n",
      "step  208/1000 | train loss 0.139217 | norm 2.3102 | lr 2.69e-05 | (106.29 ms | 38535 tok/s)\n",
      "step  209/1000 | train loss 0.139511 | norm 2.8720 | lr 2.69e-05 | (106.20 ms | 38570 tok/s)\n",
      "step  210/1000 | train loss 0.137954 | norm 1.6120 | lr 2.69e-05 | (108.04 ms | 37912 tok/s)\n",
      "step  211/1000 | train loss 0.138310 | norm 2.7054 | lr 2.69e-05 | (107.30 ms | 38174 tok/s)\n",
      "step  212/1000 | train loss 0.138123 | norm 2.2123 | lr 2.68e-05 | (106.36 ms | 38511 tok/s)\n",
      "step  213/1000 | train loss 0.137995 | norm 2.5487 | lr 2.68e-05 | (107.54 ms | 38087 tok/s)\n",
      "step  214/1000 | train loss 0.137264 | norm 2.3506 | lr 2.68e-05 | (108.33 ms | 37809 tok/s)\n",
      "step  215/1000 | train loss 0.136450 | norm 2.1586 | lr 2.67e-05 | (107.11 ms | 38242 tok/s)\n",
      "step  216/1000 | train loss 0.136232 | norm 2.1307 | lr 2.67e-05 | (106.52 ms | 38452 tok/s)\n",
      "step  217/1000 | train loss 0.135538 | norm 1.7970 | lr 2.67e-05 | (107.48 ms | 38110 tok/s)\n",
      "step  218/1000 | train loss 0.134851 | norm 1.8956 | lr 2.66e-05 | (107.12 ms | 38239 tok/s)\n",
      "step  219/1000 | train loss 0.135488 | norm 2.9193 | lr 2.66e-05 | (109.36 ms | 37455 tok/s)\n",
      "step  220/1000 | train loss 0.134971 | norm 2.1516 | lr 2.66e-05 | (108.29 ms | 37823 tok/s)\n",
      "step  221/1000 | train loss 0.134106 | norm 2.0162 | lr 2.66e-05 | (108.41 ms | 37783 tok/s)\n",
      "step  222/1000 | train loss 0.134254 | norm 2.3554 | lr 2.65e-05 | (107.58 ms | 38074 tok/s)\n",
      "step  223/1000 | train loss 0.133379 | norm 1.7812 | lr 2.65e-05 | (108.62 ms | 37710 tok/s)\n",
      "step  224/1000 | train loss 0.132974 | norm 2.3543 | lr 2.65e-05 | (107.07 ms | 38256 tok/s)\n",
      "step  225/1000 | train loss 0.133526 | norm 5.0703 | lr 2.64e-05 | (105.89 ms | 38682 tok/s)\n",
      "step  226/1000 | train loss 0.133320 | norm 4.1654 | lr 2.64e-05 | (106.61 ms | 38419 tok/s)\n",
      "step  227/1000 | train loss 0.135507 | norm 6.1513 | lr 2.64e-05 | (107.75 ms | 38015 tok/s)\n",
      "step  228/1000 | train loss 0.134985 | norm 4.9310 | lr 2.63e-05 | (107.12 ms | 38238 tok/s)\n",
      "step  229/1000 | train loss 0.132569 | norm 2.3823 | lr 2.63e-05 | (106.90 ms | 38318 tok/s)\n",
      "step  230/1000 | train loss 0.135832 | norm 10.0884 | lr 2.63e-05 | (126.17 ms | 32465 tok/s)\n",
      "step  231/1000 | train loss 0.136840 | norm 4.5153 | lr 2.63e-05 | (109.00 ms | 37578 tok/s)\n",
      "step  232/1000 | train loss 0.135659 | norm 5.1937 | lr 2.62e-05 | (106.77 ms | 38362 tok/s)\n",
      "step  233/1000 | train loss 0.134157 | norm 5.3657 | lr 2.62e-05 | (107.90 ms | 37961 tok/s)\n",
      "step  234/1000 | train loss 0.134234 | norm 5.2456 | lr 2.62e-05 | (109.45 ms | 37424 tok/s)\n",
      "step  235/1000 | train loss 0.135463 | norm 8.9326 | lr 2.61e-05 | (106.81 ms | 38347 tok/s)\n",
      "step  236/1000 | train loss 0.135402 | norm 3.8541 | lr 2.61e-05 | (106.74 ms | 38375 tok/s)\n",
      "step  237/1000 | train loss 0.133180 | norm 2.4665 | lr 2.61e-05 | (107.72 ms | 38023 tok/s)\n",
      "step  238/1000 | train loss 0.133699 | norm 5.3064 | lr 2.60e-05 | (107.03 ms | 38269 tok/s)\n",
      "step  239/1000 | train loss 0.135103 | norm 4.2426 | lr 2.60e-05 | (109.23 ms | 37499 tok/s)\n",
      "step  240/1000 | train loss 0.133105 | norm 2.4996 | lr 2.60e-05 | (108.58 ms | 37724 tok/s)\n",
      "step  241/1000 | train loss 0.132296 | norm 2.6407 | lr 2.59e-05 | (106.70 ms | 38387 tok/s)\n",
      "step  242/1000 | train loss 0.131792 | norm 2.3024 | lr 2.59e-05 | (106.24 ms | 38554 tok/s)\n",
      "step  243/1000 | train loss 0.130903 | norm 2.0710 | lr 2.59e-05 | (110.17 ms | 37178 tok/s)\n",
      "step  244/1000 | train loss 0.130394 | norm 1.8814 | lr 2.58e-05 | (107.19 ms | 38211 tok/s)\n",
      "step  245/1000 | train loss 0.130117 | norm 2.2434 | lr 2.58e-05 | (107.01 ms | 38278 tok/s)\n",
      "step  246/1000 | train loss 0.130396 | norm 2.5620 | lr 2.58e-05 | (107.12 ms | 38236 tok/s)\n",
      "step  247/1000 | train loss 0.129618 | norm 1.8039 | lr 2.57e-05 | (106.39 ms | 38500 tok/s)\n",
      "step  248/1000 | train loss 0.128728 | norm 1.6417 | lr 2.57e-05 | (108.17 ms | 37866 tok/s)\n",
      "step  249/1000 | train loss 0.128704 | norm 1.6943 | lr 2.57e-05 | (108.43 ms | 37774 tok/s)\n",
      "step  250/1000 | train loss 0.128030 | norm 1.4603 | lr 2.56e-05 | (107.61 ms | 38062 tok/s)\n",
      "step  251/1000 | train loss 0.128408 | norm 2.2207 | lr 2.56e-05 | (106.86 ms | 38331 tok/s)\n",
      "step  252/1000 | train loss 0.128088 | norm 2.0800 | lr 2.56e-05 | (108.43 ms | 37777 tok/s)\n",
      "step  253/1000 | train loss 0.127509 | norm 1.8183 | lr 2.55e-05 | (109.23 ms | 37499 tok/s)\n",
      "step  254/1000 | train loss 0.127026 | norm 1.5359 | lr 2.55e-05 | (106.33 ms | 38523 tok/s)\n",
      "step  255/1000 | train loss 0.127018 | norm 2.1203 | lr 2.55e-05 | (108.05 ms | 37910 tok/s)\n",
      "step  256/1000 | train loss 0.126488 | norm 1.5667 | lr 2.54e-05 | (108.18 ms | 37864 tok/s)\n",
      "step  257/1000 | train loss 0.126623 | norm 1.9832 | lr 2.54e-05 | (111.23 ms | 36826 tok/s)\n",
      "step  258/1000 | train loss 0.126160 | norm 1.6640 | lr 2.54e-05 | (106.19 ms | 38573 tok/s)\n",
      "step  259/1000 | train loss 0.126262 | norm 2.2194 | lr 2.53e-05 | (107.33 ms | 38162 tok/s)\n",
      "step  260/1000 | train loss 0.125497 | norm 1.6315 | lr 2.53e-05 | (106.44 ms | 38481 tok/s)\n",
      "step  261/1000 | train loss 0.124919 | norm 1.3681 | lr 2.53e-05 | (106.63 ms | 38414 tok/s)\n",
      "step  262/1000 | train loss 0.125406 | norm 1.9701 | lr 2.52e-05 | (106.21 ms | 38565 tok/s)\n",
      "step  263/1000 | train loss 0.124326 | norm 1.2561 | lr 2.52e-05 | (104.95 ms | 39028 tok/s)\n",
      "step  264/1000 | train loss 0.124507 | norm 2.3225 | lr 2.52e-05 | (107.49 ms | 38106 tok/s)\n",
      "step  265/1000 | train loss 0.123956 | norm 1.7752 | lr 2.51e-05 | (106.62 ms | 38417 tok/s)\n",
      "step  266/1000 | train loss 0.123889 | norm 2.1035 | lr 2.51e-05 | (107.71 ms | 38028 tok/s)\n",
      "step  267/1000 | train loss 0.123426 | norm 1.5605 | lr 2.51e-05 | (107.06 ms | 38258 tok/s)\n",
      "step  268/1000 | train loss 0.123316 | norm 1.8310 | lr 2.50e-05 | (107.06 ms | 38260 tok/s)\n",
      "step  269/1000 | train loss 0.122909 | norm 1.7067 | lr 2.50e-05 | (108.32 ms | 37815 tok/s)\n",
      "step  270/1000 | train loss 0.122511 | norm 1.6091 | lr 2.50e-05 | (108.49 ms | 37753 tok/s)\n",
      "step  271/1000 | train loss 0.122461 | norm 1.6978 | lr 2.49e-05 | (108.18 ms | 37863 tok/s)\n",
      "step  272/1000 | train loss 0.121746 | norm 1.4328 | lr 2.49e-05 | (107.62 ms | 38061 tok/s)\n",
      "step  273/1000 | train loss 0.122516 | norm 2.2680 | lr 2.48e-05 | (106.56 ms | 38439 tok/s)\n",
      "step  274/1000 | train loss 0.121759 | norm 1.5565 | lr 2.48e-05 | (106.58 ms | 38432 tok/s)\n",
      "step  275/1000 | train loss 0.121573 | norm 1.7666 | lr 2.48e-05 | (106.05 ms | 38623 tok/s)\n",
      "step  276/1000 | train loss 0.121664 | norm 2.0250 | lr 2.47e-05 | (106.22 ms | 38562 tok/s)\n",
      "step  277/1000 | train loss 0.121014 | norm 1.5364 | lr 2.47e-05 | (107.94 ms | 37947 tok/s)\n",
      "step  278/1000 | train loss 0.121021 | norm 1.6656 | lr 2.47e-05 | (107.66 ms | 38047 tok/s)\n",
      "step  279/1000 | train loss 0.120630 | norm 1.4686 | lr 2.46e-05 | (108.19 ms | 37861 tok/s)\n",
      "step  280/1000 | train loss 0.120263 | norm 1.3822 | lr 2.46e-05 | (106.45 ms | 38478 tok/s)\n",
      "step  281/1000 | train loss 0.120677 | norm 1.8141 | lr 2.46e-05 | (107.55 ms | 38084 tok/s)\n",
      "step  282/1000 | train loss 0.119488 | norm 0.9449 | lr 2.45e-05 | (107.48 ms | 38110 tok/s)\n",
      "step  283/1000 | train loss 0.120175 | norm 1.7379 | lr 2.45e-05 | (106.36 ms | 38510 tok/s)\n",
      "step  284/1000 | train loss 0.119414 | norm 1.2218 | lr 2.45e-05 | (107.40 ms | 38139 tok/s)\n",
      "step  285/1000 | train loss 0.119883 | norm 1.9487 | lr 2.44e-05 | (106.89 ms | 38318 tok/s)\n",
      "step  286/1000 | train loss 0.120043 | norm 1.9570 | lr 2.44e-05 | (107.29 ms | 38178 tok/s)\n",
      "step  287/1000 | train loss 0.119108 | norm 1.4830 | lr 2.43e-05 | (108.40 ms | 37787 tok/s)\n",
      "step  288/1000 | train loss 0.118809 | norm 1.2908 | lr 2.43e-05 | (106.19 ms | 38573 tok/s)\n",
      "step  289/1000 | train loss 0.118810 | norm 1.6088 | lr 2.43e-05 | (107.65 ms | 38050 tok/s)\n",
      "step  290/1000 | train loss 0.118488 | norm 1.5559 | lr 2.42e-05 | (106.14 ms | 38590 tok/s)\n",
      "step  291/1000 | train loss 0.118579 | norm 1.7174 | lr 2.42e-05 | (107.69 ms | 38036 tok/s)\n",
      "step  292/1000 | train loss 0.117977 | norm 1.3194 | lr 2.42e-05 | (106.27 ms | 38543 tok/s)\n",
      "step  293/1000 | train loss 0.118004 | norm 1.6384 | lr 2.41e-05 | (105.83 ms | 38702 tok/s)\n",
      "step  294/1000 | train loss 0.117705 | norm 1.4258 | lr 2.41e-05 | (106.19 ms | 38574 tok/s)\n",
      "step  295/1000 | train loss 0.117590 | norm 1.4575 | lr 2.40e-05 | (109.14 ms | 37528 tok/s)\n",
      "step  296/1000 | train loss 0.117137 | norm 1.1649 | lr 2.40e-05 | (107.50 ms | 38101 tok/s)\n",
      "step  297/1000 | train loss 0.117346 | norm 1.7332 | lr 2.40e-05 | (107.55 ms | 38085 tok/s)\n",
      "step  298/1000 | train loss 0.116936 | norm 1.3573 | lr 2.39e-05 | (105.34 ms | 38882 tok/s)\n",
      "step  299/1000 | train loss 0.116766 | norm 1.3553 | lr 2.39e-05 | (107.16 ms | 38224 tok/s)\n",
      "step  300/1000 | train loss 0.116598 | norm 1.3955 | lr 2.39e-05 | (106.48 ms | 38468 tok/s)\n",
      "step  301/1000 | train loss 0.116574 | norm 1.5602 | lr 2.38e-05 | (109.32 ms | 37468 tok/s)\n",
      "step  302/1000 | train loss 0.116269 | norm 1.2718 | lr 2.38e-05 | (109.81 ms | 37301 tok/s)\n",
      "step  303/1000 | train loss 0.116158 | norm 1.3469 | lr 2.37e-05 | (113.19 ms | 36188 tok/s)\n",
      "step  304/1000 | train loss 0.115928 | norm 1.3308 | lr 2.37e-05 | (108.74 ms | 37668 tok/s)\n",
      "step  305/1000 | train loss 0.115778 | norm 1.2369 | lr 2.37e-05 | (108.70 ms | 37682 tok/s)\n",
      "step  306/1000 | train loss 0.115609 | norm 1.2670 | lr 2.36e-05 | (107.92 ms | 37955 tok/s)\n",
      "step  307/1000 | train loss 0.115558 | norm 1.4842 | lr 2.36e-05 | (108.21 ms | 37852 tok/s)\n",
      "step  308/1000 | train loss 0.115400 | norm 1.5425 | lr 2.35e-05 | (107.32 ms | 38168 tok/s)\n",
      "step  309/1000 | train loss 0.115008 | norm 1.1843 | lr 2.35e-05 | (107.51 ms | 38099 tok/s)\n",
      "step  310/1000 | train loss 0.115104 | norm 1.4134 | lr 2.35e-05 | (107.20 ms | 38211 tok/s)\n",
      "step  311/1000 | train loss 0.114598 | norm 1.0770 | lr 2.34e-05 | (108.65 ms | 37700 tok/s)\n",
      "step  312/1000 | train loss 0.114631 | norm 1.1785 | lr 2.34e-05 | (104.97 ms | 39022 tok/s)\n",
      "step  313/1000 | train loss 0.114475 | norm 1.2084 | lr 2.34e-05 | (107.06 ms | 38258 tok/s)\n",
      "step  314/1000 | train loss 0.114240 | norm 1.2467 | lr 2.33e-05 | (108.25 ms | 37838 tok/s)\n",
      "step  315/1000 | train loss 0.114267 | norm 1.4904 | lr 2.33e-05 | (107.96 ms | 37940 tok/s)\n",
      "step  316/1000 | train loss 0.113974 | norm 1.2552 | lr 2.32e-05 | (104.58 ms | 39166 tok/s)\n",
      "step  317/1000 | train loss 0.113724 | norm 1.1540 | lr 2.32e-05 | (106.18 ms | 38574 tok/s)\n",
      "step  318/1000 | train loss 0.113790 | norm 1.4555 | lr 2.32e-05 | (106.04 ms | 38627 tok/s)\n",
      "step  319/1000 | train loss 0.113546 | norm 1.4231 | lr 2.31e-05 | (106.57 ms | 38436 tok/s)\n",
      "step  320/1000 | train loss 0.113574 | norm 1.6229 | lr 2.31e-05 | (105.46 ms | 38838 tok/s)\n",
      "step  321/1000 | train loss 0.113074 | norm 1.0685 | lr 2.30e-05 | (105.57 ms | 38797 tok/s)\n",
      "step  322/1000 | train loss 0.113390 | norm 1.6803 | lr 2.30e-05 | (105.61 ms | 38783 tok/s)\n",
      "step  323/1000 | train loss 0.112817 | norm 1.0147 | lr 2.30e-05 | (110.39 ms | 37106 tok/s)\n",
      "step  324/1000 | train loss 0.112890 | norm 1.3071 | lr 2.29e-05 | (106.19 ms | 38572 tok/s)\n",
      "step  325/1000 | train loss 0.112810 | norm 1.3967 | lr 2.29e-05 | (108.44 ms | 37773 tok/s)\n",
      "step  326/1000 | train loss 0.112473 | norm 1.1907 | lr 2.28e-05 | (112.69 ms | 36346 tok/s)\n",
      "step  327/1000 | train loss 0.112544 | norm 1.3384 | lr 2.28e-05 | (109.50 ms | 37406 tok/s)\n",
      "step  328/1000 | train loss 0.112274 | norm 1.1811 | lr 2.28e-05 | (109.37 ms | 37449 tok/s)\n",
      "step  329/1000 | train loss 0.112298 | norm 1.4999 | lr 2.27e-05 | (110.84 ms | 36956 tok/s)\n",
      "step  330/1000 | train loss 0.112161 | norm 1.4819 | lr 2.27e-05 | (107.22 ms | 38203 tok/s)\n",
      "step  331/1000 | train loss 0.111754 | norm 1.1673 | lr 2.26e-05 | (107.72 ms | 38024 tok/s)\n",
      "step  332/1000 | train loss 0.111790 | norm 1.3880 | lr 2.26e-05 | (107.97 ms | 37937 tok/s)\n",
      "step  333/1000 | train loss 0.111575 | norm 1.2263 | lr 2.26e-05 | (110.65 ms | 37017 tok/s)\n",
      "step  334/1000 | train loss 0.111363 | norm 1.2118 | lr 2.25e-05 | (108.93 ms | 37601 tok/s)\n",
      "step  335/1000 | train loss 0.111408 | norm 1.3133 | lr 2.25e-05 | (112.15 ms | 36523 tok/s)\n",
      "step  336/1000 | train loss 0.110958 | norm 1.0339 | lr 2.24e-05 | (112.13 ms | 36530 tok/s)\n",
      "step  337/1000 | train loss 0.111360 | norm 1.7022 | lr 2.24e-05 | (108.87 ms | 37623 tok/s)\n",
      "step  338/1000 | train loss 0.110780 | norm 0.9895 | lr 2.24e-05 | (107.96 ms | 37941 tok/s)\n",
      "step  339/1000 | train loss 0.110783 | norm 1.1126 | lr 2.23e-05 | (111.98 ms | 36577 tok/s)\n",
      "step  340/1000 | train loss 0.110834 | norm 1.2208 | lr 2.23e-05 | (112.04 ms | 36557 tok/s)\n",
      "step  341/1000 | train loss 0.110377 | norm 1.0579 | lr 2.22e-05 | (110.60 ms | 37034 tok/s)\n",
      "step  342/1000 | train loss 0.110709 | norm 1.3947 | lr 2.22e-05 | (108.98 ms | 37584 tok/s)\n",
      "step  343/1000 | train loss 0.110084 | norm 0.9601 | lr 2.21e-05 | (108.65 ms | 37699 tok/s)\n",
      "step  344/1000 | train loss 0.110757 | norm 1.5581 | lr 2.21e-05 | (110.80 ms | 36969 tok/s)\n",
      "step  345/1000 | train loss 0.110140 | norm 1.0520 | lr 2.21e-05 | (109.95 ms | 37253 tok/s)\n",
      "step  346/1000 | train loss 0.110570 | norm 1.8816 | lr 2.20e-05 | (108.76 ms | 37660 tok/s)\n",
      "step  347/1000 | train loss 0.110475 | norm 1.2886 | lr 2.20e-05 | (108.16 ms | 37871 tok/s)\n",
      "step  348/1000 | train loss 0.110050 | norm 1.3637 | lr 2.19e-05 | (108.01 ms | 37921 tok/s)\n",
      "step  349/1000 | train loss 0.110050 | norm 1.4372 | lr 2.19e-05 | (108.51 ms | 37748 tok/s)\n",
      "step  350/1000 | train loss 0.109810 | norm 1.1717 | lr 2.19e-05 | (110.42 ms | 37093 tok/s)\n",
      "step  351/1000 | train loss 0.109641 | norm 1.3468 | lr 2.18e-05 | (108.02 ms | 37920 tok/s)\n",
      "step  352/1000 | train loss 0.109424 | norm 1.2687 | lr 2.18e-05 | (108.04 ms | 37913 tok/s)\n",
      "step  353/1000 | train loss 0.109285 | norm 1.2160 | lr 2.17e-05 | (106.58 ms | 38432 tok/s)\n",
      "step  354/1000 | train loss 0.109008 | norm 1.1135 | lr 2.17e-05 | (106.91 ms | 38312 tok/s)\n",
      "step  355/1000 | train loss 0.109262 | norm 1.5691 | lr 2.16e-05 | (106.98 ms | 38287 tok/s)\n",
      "step  356/1000 | train loss 0.108876 | norm 1.2724 | lr 2.16e-05 | (110.11 ms | 37198 tok/s)\n",
      "step  357/1000 | train loss 0.108482 | norm 1.0947 | lr 2.16e-05 | (109.43 ms | 37431 tok/s)\n",
      "step  358/1000 | train loss 0.108769 | norm 1.4990 | lr 2.15e-05 | (108.55 ms | 37733 tok/s)\n",
      "step  359/1000 | train loss 0.108167 | norm 0.9881 | lr 2.15e-05 | (108.73 ms | 37673 tok/s)\n",
      "step  360/1000 | train loss 0.108379 | norm 4.1219 | lr 2.14e-05 | (108.96 ms | 37593 tok/s)\n",
      "step  361/1000 | train loss 0.108910 | norm 1.6709 | lr 2.14e-05 | (108.11 ms | 37888 tok/s)\n",
      "step  362/1000 | train loss 0.109042 | norm 3.5350 | lr 2.13e-05 | (108.75 ms | 37664 tok/s)\n",
      "step  363/1000 | train loss 0.108214 | norm 1.6961 | lr 2.13e-05 | (106.33 ms | 38522 tok/s)\n",
      "step  364/1000 | train loss 0.108033 | norm 2.6302 | lr 2.13e-05 | (107.42 ms | 38132 tok/s)\n",
      "step  365/1000 | train loss 0.108512 | norm 1.9575 | lr 2.12e-05 | (105.49 ms | 38829 tok/s)\n",
      "step  366/1000 | train loss 0.107561 | norm 1.3572 | lr 2.12e-05 | (106.97 ms | 38290 tok/s)\n",
      "step  367/1000 | train loss 0.107249 | norm 1.2301 | lr 2.11e-05 | (106.83 ms | 38342 tok/s)\n",
      "step  368/1000 | train loss 0.107437 | norm 1.5112 | lr 2.11e-05 | (107.47 ms | 38113 tok/s)\n",
      "step  369/1000 | train loss 0.107000 | norm 1.2845 | lr 2.10e-05 | (108.96 ms | 37591 tok/s)\n",
      "step  370/1000 | train loss 0.106711 | norm 1.2244 | lr 2.10e-05 | (108.60 ms | 37715 tok/s)\n",
      "step  371/1000 | train loss 0.106641 | norm 1.2325 | lr 2.10e-05 | (108.86 ms | 37625 tok/s)\n",
      "step  372/1000 | train loss 0.106520 | norm 1.3199 | lr 2.09e-05 | (107.22 ms | 38200 tok/s)\n",
      "step  373/1000 | train loss 0.106152 | norm 0.9205 | lr 2.09e-05 | (108.20 ms | 37857 tok/s)\n",
      "step  374/1000 | train loss 0.106162 | norm 1.1699 | lr 2.08e-05 | (107.93 ms | 37950 tok/s)\n",
      "step  375/1000 | train loss 0.105933 | norm 1.0504 | lr 2.08e-05 | (106.95 ms | 38300 tok/s)\n",
      "step  376/1000 | train loss 0.105775 | norm 0.9332 | lr 2.07e-05 | (107.27 ms | 38185 tok/s)\n",
      "step  377/1000 | train loss 0.105701 | norm 1.0820 | lr 2.07e-05 | (109.73 ms | 37328 tok/s)\n",
      "step  378/1000 | train loss 0.105618 | norm 1.2124 | lr 2.07e-05 | (107.50 ms | 38101 tok/s)\n",
      "step  379/1000 | train loss 0.105530 | norm 1.1406 | lr 2.06e-05 | (109.24 ms | 37494 tok/s)\n",
      "step  380/1000 | train loss 0.105264 | norm 1.0308 | lr 2.06e-05 | (108.40 ms | 37785 tok/s)\n",
      "step  381/1000 | train loss 0.105275 | norm 1.2583 | lr 2.05e-05 | (108.05 ms | 37909 tok/s)\n",
      "step  382/1000 | train loss 0.105008 | norm 0.9376 | lr 2.05e-05 | (107.88 ms | 37969 tok/s)\n",
      "step  383/1000 | train loss 0.104952 | norm 1.0742 | lr 2.04e-05 | (107.89 ms | 37963 tok/s)\n",
      "step  384/1000 | train loss 0.104734 | norm 0.9566 | lr 2.04e-05 | (137.41 ms | 29809 tok/s)\n",
      "step  385/1000 | train loss 0.104732 | norm 1.0686 | lr 2.03e-05 | (108.28 ms | 37829 tok/s)\n",
      "step  386/1000 | train loss 0.104490 | norm 0.9297 | lr 2.03e-05 | (107.60 ms | 38068 tok/s)\n",
      "step  387/1000 | train loss 0.104309 | norm 0.7674 | lr 2.03e-05 | (108.56 ms | 37730 tok/s)\n",
      "step  388/1000 | train loss 0.104180 | norm 0.7843 | lr 2.02e-05 | (108.82 ms | 37640 tok/s)\n",
      "step  389/1000 | train loss 0.104090 | norm 0.8474 | lr 2.02e-05 | (108.83 ms | 37636 tok/s)\n",
      "step  390/1000 | train loss 0.104023 | norm 0.9488 | lr 2.01e-05 | (109.22 ms | 37502 tok/s)\n",
      "step  391/1000 | train loss 0.104083 | norm 1.2527 | lr 2.01e-05 | (108.58 ms | 37722 tok/s)\n",
      "step  392/1000 | train loss 0.103869 | norm 1.0525 | lr 2.00e-05 | (106.99 ms | 38286 tok/s)\n",
      "step  393/1000 | train loss 0.103756 | norm 1.0940 | lr 2.00e-05 | (107.51 ms | 38099 tok/s)\n",
      "step  394/1000 | train loss 0.103599 | norm 1.0248 | lr 1.99e-05 | (108.37 ms | 37797 tok/s)\n",
      "step  395/1000 | train loss 0.103512 | norm 0.9945 | lr 1.99e-05 | (108.95 ms | 37596 tok/s)\n",
      "step  396/1000 | train loss 0.103351 | norm 0.9799 | lr 1.99e-05 | (108.49 ms | 37753 tok/s)\n",
      "step  397/1000 | train loss 0.103368 | norm 1.1219 | lr 1.98e-05 | (108.82 ms | 37639 tok/s)\n",
      "step  398/1000 | train loss 0.103091 | norm 0.8832 | lr 1.98e-05 | (108.26 ms | 37836 tok/s)\n",
      "step  399/1000 | train loss 0.102952 | norm 0.8477 | lr 1.97e-05 | (106.88 ms | 38322 tok/s)\n",
      "step  400/1000 | train loss 0.102991 | norm 1.0198 | lr 1.97e-05 | (107.52 ms | 38094 tok/s)\n",
      "step  401/1000 | train loss 0.102846 | norm 1.0781 | lr 1.96e-05 | (107.93 ms | 37952 tok/s)\n",
      "step  402/1000 | train loss 0.102746 | norm 1.0549 | lr 1.96e-05 | (107.88 ms | 37967 tok/s)\n",
      "step  403/1000 | train loss 0.102667 | norm 1.0791 | lr 1.95e-05 | (106.76 ms | 38365 tok/s)\n",
      "step  404/1000 | train loss 0.102469 | norm 0.9027 | lr 1.95e-05 | (107.75 ms | 38016 tok/s)\n",
      "step  405/1000 | train loss 0.102340 | norm 0.8211 | lr 1.95e-05 | (108.28 ms | 37829 tok/s)\n",
      "step  406/1000 | train loss 0.102307 | norm 0.9294 | lr 1.94e-05 | (107.96 ms | 37941 tok/s)\n",
      "step  407/1000 | train loss 0.102216 | norm 0.9501 | lr 1.94e-05 | (106.75 ms | 38371 tok/s)\n",
      "step  408/1000 | train loss 0.102017 | norm 0.7726 | lr 1.93e-05 | (107.86 ms | 37975 tok/s)\n",
      "step  409/1000 | train loss 0.101853 | norm 0.7053 | lr 1.93e-05 | (105.79 ms | 38720 tok/s)\n",
      "step  410/1000 | train loss 0.101890 | norm 0.9063 | lr 1.92e-05 | (108.79 ms | 37651 tok/s)\n",
      "step  411/1000 | train loss 0.101830 | norm 1.0033 | lr 1.92e-05 | (107.70 ms | 38030 tok/s)\n",
      "step  412/1000 | train loss 0.101789 | norm 1.0525 | lr 1.91e-05 | (107.11 ms | 38241 tok/s)\n",
      "step  413/1000 | train loss 0.101553 | norm 0.9222 | lr 1.91e-05 | (106.05 ms | 38624 tok/s)\n",
      "step  414/1000 | train loss 0.101606 | norm 1.1111 | lr 1.91e-05 | (106.52 ms | 38452 tok/s)\n",
      "step  415/1000 | train loss 0.101430 | norm 1.0252 | lr 1.90e-05 | (107.40 ms | 38136 tok/s)\n",
      "step  416/1000 | train loss 0.101435 | norm 1.1814 | lr 1.90e-05 | (108.21 ms | 37853 tok/s)\n",
      "step  417/1000 | train loss 0.101231 | norm 0.9163 | lr 1.89e-05 | (107.16 ms | 38224 tok/s)\n",
      "step  418/1000 | train loss 0.101033 | norm 0.7479 | lr 1.89e-05 | (106.36 ms | 38510 tok/s)\n",
      "step  419/1000 | train loss 0.101007 | norm 0.8583 | lr 1.88e-05 | (107.40 ms | 38138 tok/s)\n",
      "step  420/1000 | train loss 0.100950 | norm 0.9057 | lr 1.88e-05 | (106.35 ms | 38514 tok/s)\n",
      "step  421/1000 | train loss 0.100848 | norm 0.9422 | lr 1.87e-05 | (109.78 ms | 37311 tok/s)\n",
      "step  422/1000 | train loss 0.100875 | norm 1.1441 | lr 1.87e-05 | (107.98 ms | 37933 tok/s)\n",
      "step  423/1000 | train loss 0.100644 | norm 0.8652 | lr 1.86e-05 | (107.50 ms | 38102 tok/s)\n",
      "step  424/1000 | train loss 0.100603 | norm 0.9214 | lr 1.86e-05 | (110.84 ms | 36953 tok/s)\n",
      "step  425/1000 | train loss 0.100544 | norm 0.9564 | lr 1.85e-05 | (109.06 ms | 37556 tok/s)\n",
      "step  426/1000 | train loss 0.100328 | norm 0.8313 | lr 1.85e-05 | (106.24 ms | 38556 tok/s)\n",
      "step  427/1000 | train loss 0.100392 | norm 0.9675 | lr 1.85e-05 | (108.35 ms | 37804 tok/s)\n",
      "step  428/1000 | train loss 0.100231 | norm 0.9316 | lr 1.84e-05 | (107.48 ms | 38110 tok/s)\n",
      "step  429/1000 | train loss 0.100091 | norm 0.8157 | lr 1.84e-05 | (107.23 ms | 38198 tok/s)\n",
      "step  430/1000 | train loss 0.099995 | norm 0.8230 | lr 1.83e-05 | (108.34 ms | 37806 tok/s)\n",
      "step  431/1000 | train loss 0.099966 | norm 0.9073 | lr 1.83e-05 | (107.47 ms | 38114 tok/s)\n",
      "step  432/1000 | train loss 0.099864 | norm 0.9640 | lr 1.82e-05 | (109.02 ms | 37570 tok/s)\n",
      "step  433/1000 | train loss 0.099873 | norm 1.0623 | lr 1.82e-05 | (108.59 ms | 37719 tok/s)\n",
      "step  434/1000 | train loss 0.099667 | norm 0.9212 | lr 1.81e-05 | (109.27 ms | 37485 tok/s)\n",
      "step  435/1000 | train loss 0.099632 | norm 0.9780 | lr 1.81e-05 | (107.54 ms | 38087 tok/s)\n",
      "step  436/1000 | train loss 0.099526 | norm 0.9286 | lr 1.80e-05 | (108.01 ms | 37923 tok/s)\n",
      "step  437/1000 | train loss 0.099376 | norm 0.8305 | lr 1.80e-05 | (108.42 ms | 37778 tok/s)\n",
      "step  438/1000 | train loss 0.099310 | norm 0.8320 | lr 1.80e-05 | (107.61 ms | 38065 tok/s)\n",
      "step  439/1000 | train loss 0.099201 | norm 0.7660 | lr 1.79e-05 | (109.84 ms | 37292 tok/s)\n",
      "step  440/1000 | train loss 0.099164 | norm 0.8921 | lr 1.79e-05 | (108.34 ms | 37807 tok/s)\n",
      "step  441/1000 | train loss 0.099207 | norm 1.0442 | lr 1.78e-05 | (108.23 ms | 37844 tok/s)\n",
      "step  442/1000 | train loss 0.098946 | norm 0.7893 | lr 1.78e-05 | (109.77 ms | 37313 tok/s)\n",
      "step  443/1000 | train loss 0.098841 | norm 0.6891 | lr 1.77e-05 | (109.64 ms | 37357 tok/s)\n",
      "step  444/1000 | train loss 0.098735 | norm 0.6439 | lr 1.77e-05 | (107.52 ms | 38096 tok/s)\n",
      "step  445/1000 | train loss 0.098674 | norm 0.7174 | lr 1.76e-05 | (107.77 ms | 38008 tok/s)\n",
      "step  446/1000 | train loss 0.098668 | norm 0.8268 | lr 1.76e-05 | (107.30 ms | 38173 tok/s)\n",
      "step  447/1000 | train loss 0.098458 | norm 0.6318 | lr 1.75e-05 | (107.66 ms | 38045 tok/s)\n",
      "step  448/1000 | train loss 0.098413 | norm 0.6659 | lr 1.75e-05 | (108.69 ms | 37684 tok/s)\n",
      "step  449/1000 | train loss 0.098343 | norm 0.7042 | lr 1.74e-05 | (107.68 ms | 38040 tok/s)\n",
      "step  450/1000 | train loss 0.098264 | norm 0.7010 | lr 1.74e-05 | (107.83 ms | 37987 tok/s)\n",
      "step  451/1000 | train loss 0.098206 | norm 0.7408 | lr 1.73e-05 | (107.30 ms | 38172 tok/s)\n",
      "step  452/1000 | train loss 0.098094 | norm 0.6608 | lr 1.73e-05 | (107.66 ms | 38044 tok/s)\n",
      "step  453/1000 | train loss 0.097947 | norm 0.5865 | lr 1.73e-05 | (107.73 ms | 38022 tok/s)\n",
      "step  454/1000 | train loss 0.097979 | norm 0.7555 | lr 1.72e-05 | (105.39 ms | 38864 tok/s)\n",
      "step  455/1000 | train loss 0.097875 | norm 0.7817 | lr 1.72e-05 | (107.26 ms | 38188 tok/s)\n",
      "step  456/1000 | train loss 0.098001 | norm 1.1504 | lr 1.71e-05 | (107.12 ms | 38238 tok/s)\n",
      "step  457/1000 | train loss 0.098031 | norm 1.3087 | lr 1.71e-05 | (108.79 ms | 37649 tok/s)\n",
      "step  458/1000 | train loss 0.097687 | norm 0.8610 | lr 1.70e-05 | (107.91 ms | 37959 tok/s)\n",
      "step  459/1000 | train loss 0.097739 | norm 1.0307 | lr 1.70e-05 | (107.67 ms | 38041 tok/s)\n",
      "step  460/1000 | train loss 0.097680 | norm 1.0694 | lr 1.69e-05 | (109.12 ms | 37537 tok/s)\n",
      "step  461/1000 | train loss 0.097499 | norm 0.8486 | lr 1.69e-05 | (107.30 ms | 38175 tok/s)\n",
      "step  462/1000 | train loss 0.097388 | norm 0.8136 | lr 1.68e-05 | (106.06 ms | 38621 tok/s)\n",
      "step  463/1000 | train loss 0.097438 | norm 1.0105 | lr 1.68e-05 | (108.11 ms | 37888 tok/s)\n",
      "step  464/1000 | train loss 0.097391 | norm 1.0446 | lr 1.67e-05 | (107.69 ms | 38035 tok/s)\n",
      "step  465/1000 | train loss 0.097188 | norm 0.8791 | lr 1.67e-05 | (108.17 ms | 37867 tok/s)\n",
      "step  466/1000 | train loss 0.097222 | norm 1.0323 | lr 1.66e-05 | (107.57 ms | 38078 tok/s)\n",
      "step  467/1000 | train loss 0.097191 | norm 0.9988 | lr 1.66e-05 | (107.54 ms | 38089 tok/s)\n",
      "step  468/1000 | train loss 0.096941 | norm 0.7333 | lr 1.66e-05 | (108.64 ms | 37701 tok/s)\n",
      "step  469/1000 | train loss 0.096869 | norm 0.7451 | lr 1.65e-05 | (106.67 ms | 38398 tok/s)\n",
      "step  470/1000 | train loss 0.096881 | norm 0.8694 | lr 1.65e-05 | (107.84 ms | 37984 tok/s)\n",
      "step  471/1000 | train loss 0.096760 | norm 0.7823 | lr 1.64e-05 | (107.52 ms | 38095 tok/s)\n",
      "step  472/1000 | train loss 0.096639 | norm 0.7252 | lr 1.64e-05 | (107.87 ms | 37970 tok/s)\n",
      "step  473/1000 | train loss 0.096621 | norm 0.7526 | lr 1.63e-05 | (107.23 ms | 38198 tok/s)\n",
      "step  474/1000 | train loss 0.096423 | norm 0.5685 | lr 1.63e-05 | (107.42 ms | 38130 tok/s)\n",
      "step  475/1000 | train loss 0.096458 | norm 0.7412 | lr 1.62e-05 | (106.57 ms | 38436 tok/s)\n",
      "step  476/1000 | train loss 0.096361 | norm 0.7390 | lr 1.62e-05 | (107.32 ms | 38168 tok/s)\n",
      "step  477/1000 | train loss 0.096272 | norm 0.7110 | lr 1.61e-05 | (107.07 ms | 38255 tok/s)\n",
      "step  478/1000 | train loss 0.096305 | norm 0.9008 | lr 1.61e-05 | (106.76 ms | 38367 tok/s)\n",
      "step  479/1000 | train loss 0.096266 | norm 0.9795 | lr 1.60e-05 | (105.66 ms | 38767 tok/s)\n",
      "step  480/1000 | train loss 0.096220 | norm 1.0197 | lr 1.60e-05 | (107.13 ms | 38232 tok/s)\n",
      "step  481/1000 | train loss 0.096128 | norm 0.9557 | lr 1.59e-05 | (107.67 ms | 38044 tok/s)\n",
      "step  482/1000 | train loss 0.095982 | norm 0.8201 | lr 1.59e-05 | (108.12 ms | 37885 tok/s)\n",
      "step  483/1000 | train loss 0.095887 | norm 0.7172 | lr 1.58e-05 | (106.48 ms | 38468 tok/s)\n",
      "step  484/1000 | train loss 0.095799 | norm 0.7041 | lr 1.58e-05 | (107.16 ms | 38222 tok/s)\n",
      "step  485/1000 | train loss 0.095761 | norm 0.7427 | lr 1.58e-05 | (108.39 ms | 37789 tok/s)\n",
      "step  486/1000 | train loss 0.095662 | norm 0.7087 | lr 1.57e-05 | (107.86 ms | 37974 tok/s)\n",
      "step  487/1000 | train loss 0.095577 | norm 0.6527 | lr 1.57e-05 | (108.41 ms | 37782 tok/s)\n",
      "step  488/1000 | train loss 0.095463 | norm 0.5654 | lr 1.56e-05 | (107.84 ms | 37983 tok/s)\n",
      "step  489/1000 | train loss 0.095440 | norm 0.6260 | lr 1.56e-05 | (110.12 ms | 37194 tok/s)\n",
      "step  490/1000 | train loss 0.095358 | norm 0.6500 | lr 1.55e-05 | (109.11 ms | 37540 tok/s)\n",
      "step  491/1000 | train loss 0.095347 | norm 0.7250 | lr 1.55e-05 | (108.63 ms | 37706 tok/s)\n",
      "step  492/1000 | train loss 0.095237 | norm 0.6710 | lr 1.54e-05 | (108.87 ms | 37622 tok/s)\n",
      "step  493/1000 | train loss 0.095170 | norm 0.6400 | lr 1.54e-05 | (109.63 ms | 37363 tok/s)\n",
      "step  494/1000 | train loss 0.095130 | norm 0.7160 | lr 1.53e-05 | (107.22 ms | 38201 tok/s)\n",
      "step  495/1000 | train loss 0.095122 | norm 0.8428 | lr 1.53e-05 | (110.29 ms | 37137 tok/s)\n",
      "step  496/1000 | train loss 0.095060 | norm 0.8319 | lr 1.52e-05 | (107.91 ms | 37956 tok/s)\n",
      "step  497/1000 | train loss 0.094911 | norm 0.7027 | lr 1.52e-05 | (109.86 ms | 37284 tok/s)\n",
      "step  498/1000 | train loss 0.094856 | norm 0.7148 | lr 1.51e-05 | (108.95 ms | 37595 tok/s)\n",
      "step  499/1000 | train loss 0.094923 | norm 0.8818 | lr 1.51e-05 | (108.91 ms | 37610 tok/s)\n",
      "step  500/1000 | train loss 0.094845 | norm 0.8989 | lr 1.50e-05 | (108.89 ms | 37614 tok/s)\n",
      "step  501/1000 | train loss 0.094728 | norm 0.7721 | lr 1.50e-05 | (113.38 ms | 36127 tok/s)\n",
      "step  502/1000 | train loss 0.094639 | norm 0.6896 | lr 1.50e-05 | (111.23 ms | 36825 tok/s)\n",
      "step  503/1000 | train loss 0.094570 | norm 0.7102 | lr 1.49e-05 | (111.99 ms | 36574 tok/s)\n",
      "step  504/1000 | train loss 0.094549 | norm 0.7553 | lr 1.49e-05 | (110.70 ms | 37000 tok/s)\n",
      "step  505/1000 | train loss 0.094413 | norm 0.6510 | lr 1.48e-05 | (107.96 ms | 37941 tok/s)\n",
      "step  506/1000 | train loss 0.094378 | norm 0.6629 | lr 1.48e-05 | (107.65 ms | 38050 tok/s)\n",
      "step  507/1000 | train loss 0.094295 | norm 0.6418 | lr 1.47e-05 | (107.91 ms | 37957 tok/s)\n",
      "step  508/1000 | train loss 0.094242 | norm 0.6143 | lr 1.47e-05 | (108.98 ms | 37586 tok/s)\n",
      "step  509/1000 | train loss 0.094117 | norm 0.5308 | lr 1.46e-05 | (109.39 ms | 37445 tok/s)\n",
      "step  510/1000 | train loss 0.094152 | norm 0.6909 | lr 1.46e-05 | (107.69 ms | 38035 tok/s)\n",
      "step  511/1000 | train loss 0.094043 | norm 0.6433 | lr 1.45e-05 | (108.56 ms | 37729 tok/s)\n",
      "step  512/1000 | train loss 0.094004 | norm 0.6450 | lr 1.45e-05 | (110.41 ms | 37099 tok/s)\n",
      "step  513/1000 | train loss 0.093922 | norm 0.6556 | lr 1.44e-05 | (108.90 ms | 37613 tok/s)\n",
      "step  514/1000 | train loss 0.093904 | norm 0.7447 | lr 1.44e-05 | (110.47 ms | 37077 tok/s)\n",
      "step  515/1000 | train loss 0.093873 | norm 0.7847 | lr 1.43e-05 | (109.43 ms | 37430 tok/s)\n",
      "step  516/1000 | train loss 0.093791 | norm 0.7615 | lr 1.43e-05 | (111.60 ms | 36702 tok/s)\n",
      "step  517/1000 | train loss 0.093751 | norm 0.7934 | lr 1.42e-05 | (109.77 ms | 37314 tok/s)\n",
      "step  518/1000 | train loss 0.093702 | norm 0.7735 | lr 1.42e-05 | (108.35 ms | 37803 tok/s)\n",
      "step  519/1000 | train loss 0.093604 | norm 0.6951 | lr 1.42e-05 | (107.08 ms | 38251 tok/s)\n",
      "step  520/1000 | train loss 0.093568 | norm 0.7422 | lr 1.41e-05 | (108.45 ms | 37767 tok/s)\n",
      "step  521/1000 | train loss 0.093567 | norm 0.8375 | lr 1.41e-05 | (108.34 ms | 37807 tok/s)\n",
      "step  522/1000 | train loss 0.093486 | norm 0.8057 | lr 1.40e-05 | (108.47 ms | 37760 tok/s)\n",
      "step  523/1000 | train loss 0.093387 | norm 0.7559 | lr 1.40e-05 | (109.29 ms | 37478 tok/s)\n",
      "step  524/1000 | train loss 0.093387 | norm 0.8071 | lr 1.39e-05 | (108.13 ms | 37881 tok/s)\n",
      "step  525/1000 | train loss 0.093250 | norm 0.6771 | lr 1.39e-05 | (108.20 ms | 37857 tok/s)\n",
      "step  526/1000 | train loss 0.093203 | norm 0.6222 | lr 1.38e-05 | (109.07 ms | 37555 tok/s)\n",
      "step  527/1000 | train loss 0.093122 | norm 0.5663 | lr 1.38e-05 | (106.97 ms | 38293 tok/s)\n",
      "step  528/1000 | train loss 0.093019 | norm 0.5034 | lr 1.37e-05 | (107.85 ms | 37978 tok/s)\n",
      "step  529/1000 | train loss 0.093058 | norm 0.7036 | lr 1.37e-05 | (108.13 ms | 37879 tok/s)\n",
      "step  530/1000 | train loss 0.093005 | norm 0.7202 | lr 1.36e-05 | (108.91 ms | 37610 tok/s)\n",
      "step  531/1000 | train loss 0.092879 | norm 0.5521 | lr 1.36e-05 | (107.28 ms | 38179 tok/s)\n",
      "step  532/1000 | train loss 0.092821 | norm 0.5475 | lr 1.35e-05 | (107.30 ms | 38174 tok/s)\n",
      "step  533/1000 | train loss 0.092802 | norm 0.6113 | lr 1.35e-05 | (107.38 ms | 38145 tok/s)\n",
      "step  534/1000 | train loss 0.092722 | norm 0.5767 | lr 1.34e-05 | (106.70 ms | 38387 tok/s)\n",
      "step  535/1000 | train loss 0.092697 | norm 0.6090 | lr 1.34e-05 | (110.53 ms | 37056 tok/s)\n",
      "step  536/1000 | train loss 0.092627 | norm 0.5921 | lr 1.34e-05 | (108.51 ms | 37747 tok/s)\n",
      "step  537/1000 | train loss 0.092617 | norm 0.6891 | lr 1.33e-05 | (108.70 ms | 37682 tok/s)\n",
      "step  538/1000 | train loss 0.092602 | norm 0.7750 | lr 1.33e-05 | (107.63 ms | 38056 tok/s)\n",
      "step  539/1000 | train loss 0.092496 | norm 0.6703 | lr 1.32e-05 | (110.03 ms | 37228 tok/s)\n",
      "step  540/1000 | train loss 0.092414 | norm 0.5662 | lr 1.32e-05 | (110.06 ms | 37215 tok/s)\n",
      "step  541/1000 | train loss 0.092343 | norm 0.5155 | lr 1.31e-05 | (107.97 ms | 37937 tok/s)\n",
      "step  542/1000 | train loss 0.092321 | norm 0.6199 | lr 1.31e-05 | (109.91 ms | 37267 tok/s)\n",
      "step  543/1000 | train loss 0.092348 | norm 0.7713 | lr 1.30e-05 | (107.61 ms | 38063 tok/s)\n",
      "step  544/1000 | train loss 0.092258 | norm 0.7533 | lr 1.30e-05 | (110.19 ms | 37170 tok/s)\n",
      "step  545/1000 | train loss 0.092254 | norm 0.8284 | lr 1.29e-05 | (110.08 ms | 37210 tok/s)\n",
      "step  546/1000 | train loss 0.092196 | norm 0.8272 | lr 1.29e-05 | (112.11 ms | 36536 tok/s)\n",
      "step  547/1000 | train loss 0.092104 | norm 0.7345 | lr 1.28e-05 | (108.68 ms | 37688 tok/s)\n",
      "step  548/1000 | train loss 0.092067 | norm 0.7155 | lr 1.28e-05 | (106.69 ms | 38392 tok/s)\n",
      "step  549/1000 | train loss 0.091972 | norm 0.6189 | lr 1.27e-05 | (108.05 ms | 37910 tok/s)\n",
      "step  550/1000 | train loss 0.091917 | norm 0.5878 | lr 1.27e-05 | (107.77 ms | 38008 tok/s)\n",
      "step  551/1000 | train loss 0.091851 | norm 0.5425 | lr 1.27e-05 | (108.31 ms | 37819 tok/s)\n",
      "step  552/1000 | train loss 0.091783 | norm 0.5214 | lr 1.26e-05 | (108.05 ms | 37909 tok/s)\n",
      "step  553/1000 | train loss 0.091793 | norm 0.6437 | lr 1.26e-05 | (106.94 ms | 38301 tok/s)\n",
      "step  554/1000 | train loss 0.091722 | norm 0.5764 | lr 1.25e-05 | (110.18 ms | 37177 tok/s)\n",
      "step  555/1000 | train loss 0.091609 | norm 0.3989 | lr 1.25e-05 | (111.90 ms | 36604 tok/s)\n",
      "step  556/1000 | train loss 0.091594 | norm 0.4682 | lr 1.24e-05 | (112.58 ms | 36382 tok/s)\n",
      "step  557/1000 | train loss 0.091534 | norm 0.4749 | lr 1.24e-05 | (109.36 ms | 37453 tok/s)\n",
      "step  558/1000 | train loss 0.091496 | norm 0.4868 | lr 1.23e-05 | (109.65 ms | 37355 tok/s)\n",
      "step  559/1000 | train loss 0.091431 | norm 0.4545 | lr 1.23e-05 | (109.77 ms | 37314 tok/s)\n",
      "step  560/1000 | train loss 0.091403 | norm 0.5306 | lr 1.22e-05 | (108.83 ms | 37638 tok/s)\n",
      "step  561/1000 | train loss 0.091417 | norm 0.6654 | lr 1.22e-05 | (107.60 ms | 38066 tok/s)\n",
      "step  562/1000 | train loss 0.091389 | norm 0.7095 | lr 1.21e-05 | (107.30 ms | 38173 tok/s)\n",
      "step  563/1000 | train loss 0.091332 | norm 0.6972 | lr 1.21e-05 | (111.16 ms | 36848 tok/s)\n",
      "step  564/1000 | train loss 0.091236 | norm 0.5770 | lr 1.21e-05 | (107.62 ms | 38061 tok/s)\n",
      "step  565/1000 | train loss 0.091196 | norm 0.5664 | lr 1.20e-05 | (108.96 ms | 37591 tok/s)\n",
      "step  566/1000 | train loss 0.091193 | norm 0.6626 | lr 1.20e-05 | (109.07 ms | 37553 tok/s)\n",
      "step  567/1000 | train loss 0.091117 | norm 0.6318 | lr 1.19e-05 | (111.89 ms | 36606 tok/s)\n",
      "step  568/1000 | train loss 0.091076 | norm 0.6406 | lr 1.19e-05 | (107.96 ms | 37941 tok/s)\n",
      "step  569/1000 | train loss 0.091078 | norm 0.7439 | lr 1.18e-05 | (109.17 ms | 37518 tok/s)\n",
      "step  570/1000 | train loss 0.091091 | norm 0.8182 | lr 1.18e-05 | (106.84 ms | 38338 tok/s)\n",
      "step  571/1000 | train loss 0.091005 | norm 0.7871 | lr 1.17e-05 | (107.34 ms | 38160 tok/s)\n",
      "step  572/1000 | train loss 0.090965 | norm 0.7788 | lr 1.17e-05 | (106.97 ms | 38291 tok/s)\n",
      "step  573/1000 | train loss 0.090903 | norm 0.6874 | lr 1.16e-05 | (108.11 ms | 37888 tok/s)\n",
      "step  574/1000 | train loss 0.090773 | norm 0.4963 | lr 1.16e-05 | (106.27 ms | 38543 tok/s)\n",
      "step  575/1000 | train loss 0.090777 | norm 0.6176 | lr 1.15e-05 | (109.81 ms | 37301 tok/s)\n",
      "step  576/1000 | train loss 0.090733 | norm 0.6169 | lr 1.15e-05 | (110.48 ms | 37076 tok/s)\n",
      "step  577/1000 | train loss 0.090632 | norm 0.4696 | lr 1.15e-05 | (108.63 ms | 37704 tok/s)\n",
      "step  578/1000 | train loss 0.090630 | norm 0.5727 | lr 1.14e-05 | (109.20 ms | 37508 tok/s)\n",
      "step  579/1000 | train loss 0.090567 | norm 0.5016 | lr 1.14e-05 | (109.85 ms | 37287 tok/s)\n",
      "step  580/1000 | train loss 0.090477 | norm 0.3713 | lr 1.13e-05 | (106.56 ms | 38439 tok/s)\n",
      "step  581/1000 | train loss 0.090472 | norm 0.4724 | lr 1.13e-05 | (106.54 ms | 38446 tok/s)\n",
      "step  582/1000 | train loss 0.090390 | norm 0.3862 | lr 1.12e-05 | (108.45 ms | 37768 tok/s)\n",
      "step  583/1000 | train loss 0.090371 | norm 0.4310 | lr 1.12e-05 | (109.70 ms | 37339 tok/s)\n",
      "step  584/1000 | train loss 0.090314 | norm 0.4072 | lr 1.11e-05 | (109.86 ms | 37283 tok/s)\n",
      "step  585/1000 | train loss 0.090256 | norm 0.3657 | lr 1.11e-05 | (109.33 ms | 37464 tok/s)\n",
      "step  586/1000 | train loss 0.090242 | norm 0.4524 | lr 1.10e-05 | (106.68 ms | 38396 tok/s)\n",
      "step  587/1000 | train loss 0.090198 | norm 0.4693 | lr 1.10e-05 | (106.92 ms | 38310 tok/s)\n",
      "step  588/1000 | train loss 0.090181 | norm 0.5645 | lr 1.10e-05 | (108.34 ms | 37807 tok/s)\n",
      "step  589/1000 | train loss 0.090217 | norm 0.7620 | lr 1.09e-05 | (109.39 ms | 37445 tok/s)\n",
      "step  590/1000 | train loss 0.090239 | norm 0.9130 | lr 1.09e-05 | (110.53 ms | 37056 tok/s)\n",
      "step  591/1000 | train loss 0.090179 | norm 0.8676 | lr 1.08e-05 | (108.64 ms | 37704 tok/s)\n",
      "step  592/1000 | train loss 0.090056 | norm 0.6314 | lr 1.08e-05 | (110.06 ms | 37217 tok/s)\n",
      "step  593/1000 | train loss 0.089963 | norm 0.4828 | lr 1.07e-05 | (110.46 ms | 37081 tok/s)\n",
      "step  594/1000 | train loss 0.089994 | norm 0.7151 | lr 1.07e-05 | (110.58 ms | 37041 tok/s)\n",
      "step  595/1000 | train loss 0.090002 | norm 0.8062 | lr 1.06e-05 | (112.40 ms | 36443 tok/s)\n",
      "step  596/1000 | train loss 0.089825 | norm 0.4494 | lr 1.06e-05 | (108.70 ms | 37683 tok/s)\n",
      "step  597/1000 | train loss 0.089790 | norm 0.3939 | lr 1.05e-05 | (110.93 ms | 36924 tok/s)\n",
      "step  598/1000 | train loss 0.089792 | norm 0.6071 | lr 1.05e-05 | (110.92 ms | 36929 tok/s)\n",
      "step  599/1000 | train loss 0.089756 | norm 0.5849 | lr 1.05e-05 | (110.16 ms | 37181 tok/s)\n",
      "step  600/1000 | train loss 0.089659 | norm 0.3820 | lr 1.04e-05 | (109.25 ms | 37491 tok/s)\n",
      "step  601/1000 | train loss 0.089643 | norm 0.4307 | lr 1.04e-05 | (111.42 ms | 36762 tok/s)\n",
      "step  602/1000 | train loss 0.089601 | norm 0.4821 | lr 1.03e-05 | (109.49 ms | 37409 tok/s)\n",
      "step  603/1000 | train loss 0.089587 | norm 0.5059 | lr 1.03e-05 | (108.39 ms | 37790 tok/s)\n",
      "step  604/1000 | train loss 0.089502 | norm 0.3437 | lr 1.02e-05 | (108.59 ms | 37718 tok/s)\n",
      "step  605/1000 | train loss 0.089463 | norm 0.3426 | lr 1.02e-05 | (110.38 ms | 37107 tok/s)\n",
      "step  606/1000 | train loss 0.089459 | norm 0.4726 | lr 1.01e-05 | (111.20 ms | 36836 tok/s)\n",
      "step  607/1000 | train loss 0.089399 | norm 0.4227 | lr 1.01e-05 | (110.25 ms | 37151 tok/s)\n",
      "step  608/1000 | train loss 0.089349 | norm 0.3531 | lr 1.01e-05 | (108.69 ms | 37684 tok/s)\n",
      "step  609/1000 | train loss 0.089322 | norm 0.3875 | lr 1.00e-05 | (110.29 ms | 37138 tok/s)\n",
      "step  610/1000 | train loss 0.089307 | norm 0.4657 | lr 9.97e-06 | (109.98 ms | 37243 tok/s)\n",
      "step  611/1000 | train loss 0.089259 | norm 0.4705 | lr 9.92e-06 | (109.52 ms | 37400 tok/s)\n",
      "step  612/1000 | train loss 0.089260 | norm 0.5532 | lr 9.88e-06 | (109.86 ms | 37283 tok/s)\n",
      "step  613/1000 | train loss 0.089233 | norm 0.5854 | lr 9.83e-06 | (109.86 ms | 37283 tok/s)\n",
      "step  614/1000 | train loss 0.089193 | norm 0.5948 | lr 9.79e-06 | (108.98 ms | 37585 tok/s)\n",
      "step  615/1000 | train loss 0.089169 | norm 0.6093 | lr 9.74e-06 | (124.20 ms | 32980 tok/s)\n",
      "step  616/1000 | train loss 0.089128 | norm 0.6077 | lr 9.70e-06 | (110.69 ms | 37005 tok/s)\n",
      "step  617/1000 | train loss 0.089154 | norm 0.7552 | lr 9.66e-06 | (109.30 ms | 37477 tok/s)\n",
      "step  618/1000 | train loss 0.089167 | norm 0.8330 | lr 9.61e-06 | (109.37 ms | 37450 tok/s)\n",
      "step  619/1000 | train loss 0.088994 | norm 0.5247 | lr 9.57e-06 | (108.26 ms | 37834 tok/s)\n",
      "step  620/1000 | train loss 0.088945 | norm 0.4404 | lr 9.52e-06 | (108.29 ms | 37825 tok/s)\n",
      "step  621/1000 | train loss 0.088981 | norm 0.6577 | lr 9.48e-06 | (108.45 ms | 37768 tok/s)\n",
      "step  622/1000 | train loss 0.088892 | norm 0.5197 | lr 9.44e-06 | (109.19 ms | 37511 tok/s)\n",
      "step  623/1000 | train loss 0.088840 | norm 0.4362 | lr 9.39e-06 | (108.05 ms | 37909 tok/s)\n",
      "step  624/1000 | train loss 0.088833 | norm 0.5164 | lr 9.35e-06 | (138.31 ms | 29614 tok/s)\n",
      "step  625/1000 | train loss 0.088763 | norm 0.4245 | lr 9.31e-06 | (108.23 ms | 37844 tok/s)\n",
      "step  626/1000 | train loss 0.088739 | norm 0.4126 | lr 9.26e-06 | (111.71 ms | 36665 tok/s)\n",
      "step  627/1000 | train loss 0.088683 | norm 0.3721 | lr 9.22e-06 | (109.22 ms | 37503 tok/s)\n",
      "step  628/1000 | train loss 0.088667 | norm 0.4156 | lr 9.17e-06 | (109.72 ms | 37331 tok/s)\n",
      "step  629/1000 | train loss 0.088621 | norm 0.3711 | lr 9.13e-06 | (114.38 ms | 35810 tok/s)\n",
      "step  630/1000 | train loss 0.088578 | norm 0.3438 | lr 9.09e-06 | (110.42 ms | 37095 tok/s)\n",
      "step  631/1000 | train loss 0.088560 | norm 0.3905 | lr 9.04e-06 | (110.43 ms | 37090 tok/s)\n",
      "step  632/1000 | train loss 0.088504 | norm 0.3120 | lr 9.00e-06 | (109.82 ms | 37296 tok/s)\n",
      "step  633/1000 | train loss 0.088478 | norm 0.3451 | lr 8.96e-06 | (111.31 ms | 36798 tok/s)\n",
      "step  634/1000 | train loss 0.088467 | norm 0.4202 | lr 8.92e-06 | (109.03 ms | 37568 tok/s)\n",
      "step  635/1000 | train loss 0.088416 | norm 0.3904 | lr 8.87e-06 | (108.57 ms | 37726 tok/s)\n",
      "step  636/1000 | train loss 0.088424 | norm 0.5316 | lr 8.83e-06 | (109.56 ms | 37388 tok/s)\n",
      "step  637/1000 | train loss 0.088432 | norm 0.6532 | lr 8.79e-06 | (111.71 ms | 36665 tok/s)\n",
      "step  638/1000 | train loss 0.088396 | norm 0.6691 | lr 8.74e-06 | (109.98 ms | 37244 tok/s)\n",
      "step  639/1000 | train loss 0.088361 | norm 0.6329 | lr 8.70e-06 | (110.10 ms | 37202 tok/s)\n",
      "step  640/1000 | train loss 0.088270 | norm 0.4338 | lr 8.66e-06 | (109.28 ms | 37481 tok/s)\n",
      "step  641/1000 | train loss 0.088220 | norm 0.3352 | lr 8.62e-06 | (107.45 ms | 38119 tok/s)\n",
      "step  642/1000 | train loss 0.088226 | norm 0.4882 | lr 8.57e-06 | (108.85 ms | 37629 tok/s)\n",
      "step  643/1000 | train loss 0.088190 | norm 0.5014 | lr 8.53e-06 | (109.92 ms | 37263 tok/s)\n",
      "step  644/1000 | train loss 0.088139 | norm 0.4135 | lr 8.49e-06 | (110.11 ms | 37198 tok/s)\n",
      "step  645/1000 | train loss 0.088096 | norm 0.3392 | lr 8.45e-06 | (110.62 ms | 37026 tok/s)\n",
      "step  646/1000 | train loss 0.088069 | norm 0.3668 | lr 8.40e-06 | (110.26 ms | 37148 tok/s)\n",
      "step  647/1000 | train loss 0.088069 | norm 0.4649 | lr 8.36e-06 | (107.63 ms | 38055 tok/s)\n",
      "step  648/1000 | train loss 0.088008 | norm 0.3674 | lr 8.32e-06 | (108.68 ms | 37687 tok/s)\n",
      "step  649/1000 | train loss 0.087980 | norm 0.3751 | lr 8.28e-06 | (106.76 ms | 38368 tok/s)\n",
      "step  650/1000 | train loss 0.087993 | norm 0.5411 | lr 8.23e-06 | (111.71 ms | 36667 tok/s)\n",
      "step  651/1000 | train loss 0.087982 | norm 0.6068 | lr 8.19e-06 | (111.41 ms | 36765 tok/s)\n",
      "step  652/1000 | train loss 0.087935 | norm 0.5361 | lr 8.15e-06 | (110.24 ms | 37154 tok/s)\n",
      "step  653/1000 | train loss 0.087865 | norm 0.3742 | lr 8.11e-06 | (110.06 ms | 37214 tok/s)\n",
      "step  654/1000 | train loss 0.087832 | norm 0.3817 | lr 8.07e-06 | (108.62 ms | 37710 tok/s)\n",
      "step  655/1000 | train loss 0.087831 | norm 0.4886 | lr 8.02e-06 | (108.95 ms | 37596 tok/s)\n",
      "step  656/1000 | train loss 0.087777 | norm 0.3855 | lr 7.98e-06 | (108.37 ms | 37795 tok/s)\n",
      "step  657/1000 | train loss 0.087730 | norm 0.2828 | lr 7.94e-06 | (108.29 ms | 37825 tok/s)\n",
      "step  658/1000 | train loss 0.087719 | norm 0.3867 | lr 7.90e-06 | (108.14 ms | 37876 tok/s)\n",
      "step  659/1000 | train loss 0.087694 | norm 0.3915 | lr 7.86e-06 | (107.85 ms | 37980 tok/s)\n",
      "step  660/1000 | train loss 0.087640 | norm 0.2711 | lr 7.82e-06 | (107.80 ms | 37996 tok/s)\n",
      "step  661/1000 | train loss 0.087626 | norm 0.3259 | lr 7.78e-06 | (108.71 ms | 37679 tok/s)\n",
      "step  662/1000 | train loss 0.087600 | norm 0.3568 | lr 7.73e-06 | (109.12 ms | 37537 tok/s)\n",
      "step  663/1000 | train loss 0.087570 | norm 0.3536 | lr 7.69e-06 | (107.72 ms | 38025 tok/s)\n",
      "step  664/1000 | train loss 0.087551 | norm 0.3974 | lr 7.65e-06 | (108.36 ms | 37800 tok/s)\n",
      "step  665/1000 | train loss 0.087539 | norm 0.4933 | lr 7.61e-06 | (106.13 ms | 38594 tok/s)\n",
      "step  666/1000 | train loss 0.087562 | norm 0.6536 | lr 7.57e-06 | (107.33 ms | 38162 tok/s)\n",
      "step  667/1000 | train loss 0.087553 | norm 0.7028 | lr 7.53e-06 | (107.98 ms | 37934 tok/s)\n",
      "step  668/1000 | train loss 0.087486 | norm 0.5689 | lr 7.49e-06 | (108.35 ms | 37802 tok/s)\n",
      "step  669/1000 | train loss 0.087412 | norm 0.3547 | lr 7.45e-06 | (108.34 ms | 37809 tok/s)\n",
      "step  670/1000 | train loss 0.087396 | norm 0.4309 | lr 7.41e-06 | (107.87 ms | 37973 tok/s)\n",
      "step  671/1000 | train loss 0.087401 | norm 0.5532 | lr 7.37e-06 | (107.90 ms | 37961 tok/s)\n",
      "step  672/1000 | train loss 0.087342 | norm 0.4152 | lr 7.33e-06 | (107.75 ms | 38015 tok/s)\n",
      "step  673/1000 | train loss 0.087283 | norm 0.2413 | lr 7.29e-06 | (106.42 ms | 38488 tok/s)\n",
      "step  674/1000 | train loss 0.087292 | norm 0.4198 | lr 7.25e-06 | (107.44 ms | 38122 tok/s)\n",
      "step  675/1000 | train loss 0.087259 | norm 0.4115 | lr 7.21e-06 | (106.78 ms | 38360 tok/s)\n",
      "step  676/1000 | train loss 0.087216 | norm 0.2958 | lr 7.16e-06 | (107.73 ms | 38021 tok/s)\n",
      "step  677/1000 | train loss 0.087188 | norm 0.2932 | lr 7.12e-06 | (110.13 ms | 37192 tok/s)\n",
      "step  678/1000 | train loss 0.087168 | norm 0.3425 | lr 7.08e-06 | (109.48 ms | 37412 tok/s)\n",
      "step  679/1000 | train loss 0.087145 | norm 0.3364 | lr 7.04e-06 | (110.72 ms | 36996 tok/s)\n",
      "step  680/1000 | train loss 0.087105 | norm 0.2423 | lr 7.00e-06 | (109.99 ms | 37238 tok/s)\n",
      "step  681/1000 | train loss 0.087077 | norm 0.2359 | lr 6.96e-06 | (112.31 ms | 36472 tok/s)\n",
      "step  682/1000 | train loss 0.087066 | norm 0.3051 | lr 6.93e-06 | (109.31 ms | 37471 tok/s)\n",
      "step  683/1000 | train loss 0.087027 | norm 0.2437 | lr 6.89e-06 | (109.32 ms | 37469 tok/s)\n",
      "step  684/1000 | train loss 0.087002 | norm 0.2246 | lr 6.85e-06 | (107.78 ms | 38003 tok/s)\n",
      "step  685/1000 | train loss 0.086982 | norm 0.2493 | lr 6.81e-06 | (107.61 ms | 38062 tok/s)\n",
      "step  686/1000 | train loss 0.086952 | norm 0.2424 | lr 6.77e-06 | (114.37 ms | 35813 tok/s)\n",
      "step  687/1000 | train loss 0.086938 | norm 0.3010 | lr 6.73e-06 | (109.53 ms | 37397 tok/s)\n",
      "step  688/1000 | train loss 0.086933 | norm 0.3931 | lr 6.69e-06 | (108.94 ms | 37599 tok/s)\n",
      "step  689/1000 | train loss 0.086928 | norm 0.4808 | lr 6.65e-06 | (110.23 ms | 37159 tok/s)\n",
      "step  690/1000 | train loss 0.086922 | norm 0.5447 | lr 6.61e-06 | (107.49 ms | 38105 tok/s)\n",
      "step  691/1000 | train loss 0.086890 | norm 0.4999 | lr 6.57e-06 | (108.06 ms | 37906 tok/s)\n",
      "step  692/1000 | train loss 0.086828 | norm 0.3355 | lr 6.53e-06 | (109.33 ms | 37464 tok/s)\n",
      "step  693/1000 | train loss 0.086804 | norm 0.3387 | lr 6.49e-06 | (108.10 ms | 37891 tok/s)\n",
      "step  694/1000 | train loss 0.086806 | norm 0.4659 | lr 6.45e-06 | (108.93 ms | 37601 tok/s)\n",
      "step  695/1000 | train loss 0.086781 | norm 0.4462 | lr 6.42e-06 | (108.59 ms | 37719 tok/s)\n",
      "step  696/1000 | train loss 0.086725 | norm 0.3092 | lr 6.38e-06 | (107.81 ms | 37994 tok/s)\n",
      "step  697/1000 | train loss 0.086725 | norm 0.4025 | lr 6.34e-06 | (108.97 ms | 37588 tok/s)\n",
      "step  698/1000 | train loss 0.086704 | norm 0.4220 | lr 6.30e-06 | (109.87 ms | 37279 tok/s)\n",
      "step  699/1000 | train loss 0.086662 | norm 0.3061 | lr 6.26e-06 | (107.89 ms | 37965 tok/s)\n",
      "step  700/1000 | train loss 0.086628 | norm 0.2537 | lr 6.22e-06 | (108.56 ms | 37731 tok/s)\n",
      "step  701/1000 | train loss 0.086624 | norm 0.3428 | lr 6.19e-06 | (106.85 ms | 38333 tok/s)\n",
      "step  702/1000 | train loss 0.086589 | norm 0.2823 | lr 6.15e-06 | (109.00 ms | 37577 tok/s)\n",
      "step  703/1000 | train loss 0.086556 | norm 0.2002 | lr 6.11e-06 | (112.29 ms | 36477 tok/s)\n",
      "step  704/1000 | train loss 0.086545 | norm 0.2635 | lr 6.07e-06 | (110.58 ms | 37040 tok/s)\n",
      "step  705/1000 | train loss 0.086521 | norm 0.2688 | lr 6.03e-06 | (109.76 ms | 37319 tok/s)\n",
      "step  706/1000 | train loss 0.086499 | norm 0.2537 | lr 6.00e-06 | (108.98 ms | 37585 tok/s)\n",
      "step  707/1000 | train loss 0.086476 | norm 0.2632 | lr 5.96e-06 | (107.67 ms | 38043 tok/s)\n",
      "step  708/1000 | train loss 0.086465 | norm 0.3323 | lr 5.92e-06 | (107.60 ms | 38068 tok/s)\n",
      "step  709/1000 | train loss 0.086452 | norm 0.3870 | lr 5.88e-06 | (107.06 ms | 38258 tok/s)\n",
      "step  710/1000 | train loss 0.086441 | norm 0.4431 | lr 5.85e-06 | (107.70 ms | 38033 tok/s)\n",
      "step  711/1000 | train loss 0.086435 | norm 0.4908 | lr 5.81e-06 | (109.73 ms | 37328 tok/s)\n",
      "step  712/1000 | train loss 0.086393 | norm 0.4151 | lr 5.77e-06 | (107.65 ms | 38050 tok/s)\n",
      "step  713/1000 | train loss 0.086355 | norm 0.2844 | lr 5.73e-06 | (108.12 ms | 37882 tok/s)\n",
      "step  714/1000 | train loss 0.086324 | norm 0.2172 | lr 5.70e-06 | (108.93 ms | 37601 tok/s)\n",
      "step  715/1000 | train loss 0.086318 | norm 0.3219 | lr 5.66e-06 | (109.68 ms | 37346 tok/s)\n",
      "step  716/1000 | train loss 0.086304 | norm 0.3632 | lr 5.62e-06 | (107.83 ms | 37984 tok/s)\n",
      "step  717/1000 | train loss 0.086272 | norm 0.2896 | lr 5.59e-06 | (106.67 ms | 38397 tok/s)\n",
      "step  718/1000 | train loss 0.086242 | norm 0.2153 | lr 5.55e-06 | (107.22 ms | 38202 tok/s)\n",
      "step  719/1000 | train loss 0.086229 | norm 0.2496 | lr 5.51e-06 | (108.59 ms | 37719 tok/s)\n",
      "step  720/1000 | train loss 0.086204 | norm 0.2433 | lr 5.48e-06 | (111.04 ms | 36888 tok/s)\n",
      "step  721/1000 | train loss 0.086188 | norm 0.2669 | lr 5.44e-06 | (107.60 ms | 38065 tok/s)\n",
      "step  722/1000 | train loss 0.086167 | norm 0.2528 | lr 5.40e-06 | (108.32 ms | 37814 tok/s)\n",
      "step  723/1000 | train loss 0.086142 | norm 0.2033 | lr 5.37e-06 | (106.58 ms | 38431 tok/s)\n",
      "step  724/1000 | train loss 0.086123 | norm 0.2006 | lr 5.33e-06 | (108.41 ms | 37782 tok/s)\n",
      "step  725/1000 | train loss 0.086100 | norm 0.1876 | lr 5.30e-06 | (108.12 ms | 37883 tok/s)\n",
      "step  726/1000 | train loss 0.086087 | norm 0.2308 | lr 5.26e-06 | (109.13 ms | 37534 tok/s)\n",
      "step  727/1000 | train loss 0.086071 | norm 0.2614 | lr 5.22e-06 | (107.60 ms | 38068 tok/s)\n",
      "step  728/1000 | train loss 0.086056 | norm 0.2912 | lr 5.19e-06 | (107.36 ms | 38150 tok/s)\n",
      "step  729/1000 | train loss 0.086049 | norm 0.3691 | lr 5.15e-06 | (106.09 ms | 38609 tok/s)\n",
      "step  730/1000 | train loss 0.086049 | norm 0.4465 | lr 5.12e-06 | (106.59 ms | 38429 tok/s)\n",
      "step  731/1000 | train loss 0.086018 | norm 0.3862 | lr 5.08e-06 | (107.88 ms | 37968 tok/s)\n",
      "step  732/1000 | train loss 0.085979 | norm 0.2646 | lr 5.05e-06 | (106.31 ms | 38529 tok/s)\n",
      "step  733/1000 | train loss 0.085971 | norm 0.3418 | lr 5.01e-06 | (106.77 ms | 38364 tok/s)\n",
      "step  734/1000 | train loss 0.085964 | norm 0.4021 | lr 4.98e-06 | (108.66 ms | 37696 tok/s)\n",
      "step  735/1000 | train loss 0.085927 | norm 0.2856 | lr 4.94e-06 | (113.38 ms | 36127 tok/s)\n",
      "step  736/1000 | train loss 0.085900 | norm 0.2002 | lr 4.91e-06 | (109.21 ms | 37507 tok/s)\n",
      "step  737/1000 | train loss 0.085895 | norm 0.2957 | lr 4.87e-06 | (142.01 ms | 28842 tok/s)\n",
      "step  738/1000 | train loss 0.085872 | norm 0.2730 | lr 4.84e-06 | (109.30 ms | 37475 tok/s)\n",
      "step  739/1000 | train loss 0.085850 | norm 0.2219 | lr 4.80e-06 | (109.45 ms | 37424 tok/s)\n",
      "step  740/1000 | train loss 0.085836 | norm 0.2521 | lr 4.77e-06 | (108.79 ms | 37652 tok/s)\n",
      "step  741/1000 | train loss 0.085815 | norm 0.2347 | lr 4.73e-06 | (110.78 ms | 36973 tok/s)\n",
      "step  742/1000 | train loss 0.085795 | norm 0.1906 | lr 4.70e-06 | (109.96 ms | 37250 tok/s)\n",
      "step  743/1000 | train loss 0.085779 | norm 0.2016 | lr 4.67e-06 | (110.56 ms | 37046 tok/s)\n",
      "step  744/1000 | train loss 0.085761 | norm 0.2045 | lr 4.63e-06 | (110.13 ms | 37193 tok/s)\n",
      "step  745/1000 | train loss 0.085742 | norm 0.1867 | lr 4.60e-06 | (108.61 ms | 37715 tok/s)\n",
      "step  746/1000 | train loss 0.085730 | norm 0.2181 | lr 4.56e-06 | (108.74 ms | 37669 tok/s)\n",
      "step  747/1000 | train loss 0.085714 | norm 0.2316 | lr 4.53e-06 | (107.11 ms | 38243 tok/s)\n",
      "step  748/1000 | train loss 0.085702 | norm 0.2660 | lr 4.50e-06 | (108.92 ms | 37607 tok/s)\n",
      "step  749/1000 | train loss 0.085690 | norm 0.3063 | lr 4.46e-06 | (109.70 ms | 37337 tok/s)\n",
      "step  750/1000 | train loss 0.085680 | norm 0.3341 | lr 4.43e-06 | (109.11 ms | 37540 tok/s)\n",
      "step  751/1000 | train loss 0.085653 | norm 0.2877 | lr 4.40e-06 | (111.23 ms | 36824 tok/s)\n",
      "step  752/1000 | train loss 0.085636 | norm 0.2594 | lr 4.36e-06 | (107.77 ms | 38008 tok/s)\n",
      "step  753/1000 | train loss 0.085621 | norm 0.2612 | lr 4.33e-06 | (113.54 ms | 36076 tok/s)\n",
      "step  754/1000 | train loss 0.085609 | norm 0.2861 | lr 4.30e-06 | (110.09 ms | 37207 tok/s)\n",
      "step  755/1000 | train loss 0.085598 | norm 0.2946 | lr 4.26e-06 | (107.26 ms | 38188 tok/s)\n",
      "step  756/1000 | train loss 0.085567 | norm 0.1973 | lr 4.23e-06 | (106.59 ms | 38429 tok/s)\n",
      "step  757/1000 | train loss 0.085548 | norm 0.1570 | lr 4.20e-06 | (108.69 ms | 37686 tok/s)\n",
      "step  758/1000 | train loss 0.085540 | norm 0.2248 | lr 4.17e-06 | (111.25 ms | 36818 tok/s)\n",
      "step  759/1000 | train loss 0.085526 | norm 0.2331 | lr 4.13e-06 | (111.75 ms | 36652 tok/s)\n",
      "step  760/1000 | train loss 0.085505 | norm 0.1809 | lr 4.10e-06 | (108.65 ms | 37700 tok/s)\n",
      "step  761/1000 | train loss 0.085489 | norm 0.1758 | lr 4.07e-06 | (110.73 ms | 36992 tok/s)\n",
      "step  762/1000 | train loss 0.085476 | norm 0.1960 | lr 4.04e-06 | (106.21 ms | 38565 tok/s)\n",
      "step  763/1000 | train loss 0.085461 | norm 0.1950 | lr 4.00e-06 | (111.81 ms | 36633 tok/s)\n",
      "step  764/1000 | train loss 0.085446 | norm 0.1820 | lr 3.97e-06 | (111.78 ms | 36644 tok/s)\n",
      "step  765/1000 | train loss 0.085427 | norm 0.1491 | lr 3.94e-06 | (110.21 ms | 37165 tok/s)\n",
      "step  766/1000 | train loss 0.085415 | norm 0.1742 | lr 3.91e-06 | (109.70 ms | 37337 tok/s)\n",
      "step  767/1000 | train loss 0.085403 | norm 0.1927 | lr 3.88e-06 | (109.89 ms | 37275 tok/s)\n",
      "step  768/1000 | train loss 0.085388 | norm 0.1929 | lr 3.84e-06 | (111.63 ms | 36691 tok/s)\n",
      "step  769/1000 | train loss 0.085377 | norm 0.2261 | lr 3.81e-06 | (112.01 ms | 36567 tok/s)\n",
      "step  770/1000 | train loss 0.085369 | norm 0.2802 | lr 3.78e-06 | (109.18 ms | 37517 tok/s)\n",
      "step  771/1000 | train loss 0.085366 | norm 0.3512 | lr 3.75e-06 | (112.21 ms | 36504 tok/s)\n",
      "step  772/1000 | train loss 0.085354 | norm 0.3568 | lr 3.72e-06 | (108.91 ms | 37610 tok/s)\n",
      "step  773/1000 | train loss 0.085321 | norm 0.2243 | lr 3.69e-06 | (109.97 ms | 37246 tok/s)\n",
      "step  774/1000 | train loss 0.085304 | norm 0.1659 | lr 3.66e-06 | (107.55 ms | 38085 tok/s)\n",
      "step  775/1000 | train loss 0.085300 | norm 0.2627 | lr 3.63e-06 | (107.46 ms | 38116 tok/s)\n",
      "step  776/1000 | train loss 0.085286 | norm 0.2676 | lr 3.60e-06 | (109.79 ms | 37307 tok/s)\n",
      "step  777/1000 | train loss 0.085265 | norm 0.1812 | lr 3.57e-06 | (110.90 ms | 36934 tok/s)\n",
      "step  778/1000 | train loss 0.085249 | norm 0.1498 | lr 3.54e-06 | (109.97 ms | 37247 tok/s)\n",
      "step  779/1000 | train loss 0.085243 | norm 0.2206 | lr 3.51e-06 | (109.88 ms | 37277 tok/s)\n",
      "step  780/1000 | train loss 0.085228 | norm 0.2003 | lr 3.48e-06 | (110.32 ms | 37130 tok/s)\n",
      "step  781/1000 | train loss 0.085210 | norm 0.1387 | lr 3.44e-06 | (110.11 ms | 37198 tok/s)\n",
      "step  782/1000 | train loss 0.085199 | norm 0.1702 | lr 3.41e-06 | (110.14 ms | 37190 tok/s)\n",
      "step  783/1000 | train loss 0.085190 | norm 0.2092 | lr 3.39e-06 | (110.35 ms | 37117 tok/s)\n",
      "step  784/1000 | train loss 0.085173 | norm 0.1607 | lr 3.36e-06 | (109.15 ms | 37528 tok/s)\n",
      "step  785/1000 | train loss 0.085159 | norm 0.1308 | lr 3.33e-06 | (111.99 ms | 36576 tok/s)\n",
      "step  786/1000 | train loss 0.085152 | norm 0.1848 | lr 3.30e-06 | (108.06 ms | 37906 tok/s)\n",
      "step  787/1000 | train loss 0.085141 | norm 0.1987 | lr 3.27e-06 | (109.75 ms | 37323 tok/s)\n",
      "step  788/1000 | train loss 0.085126 | norm 0.1797 | lr 3.24e-06 | (108.32 ms | 37815 tok/s)\n",
      "step  789/1000 | train loss 0.085116 | norm 0.2067 | lr 3.21e-06 | (107.97 ms | 37936 tok/s)\n",
      "step  790/1000 | train loss 0.085108 | norm 0.2442 | lr 3.18e-06 | (109.72 ms | 37331 tok/s)\n",
      "step  791/1000 | train loss 0.085092 | norm 0.2060 | lr 3.15e-06 | (111.04 ms | 36886 tok/s)\n",
      "step  792/1000 | train loss 0.085075 | norm 0.1387 | lr 3.12e-06 | (109.08 ms | 37549 tok/s)\n",
      "step  793/1000 | train loss 0.085064 | norm 0.1444 | lr 3.09e-06 | (111.93 ms | 36593 tok/s)\n",
      "step  794/1000 | train loss 0.085054 | norm 0.1668 | lr 3.06e-06 | (110.05 ms | 37220 tok/s)\n",
      "step  795/1000 | train loss 0.085042 | norm 0.1577 | lr 3.04e-06 | (110.49 ms | 37071 tok/s)\n",
      "step  796/1000 | train loss 0.085031 | norm 0.1488 | lr 3.01e-06 | (109.21 ms | 37505 tok/s)\n",
      "step  797/1000 | train loss 0.085021 | norm 0.1648 | lr 2.98e-06 | (109.05 ms | 37562 tok/s)\n",
      "step  798/1000 | train loss 0.085008 | norm 0.1479 | lr 2.95e-06 | (107.48 ms | 38110 tok/s)\n",
      "step  799/1000 | train loss 0.084995 | norm 0.1178 | lr 2.92e-06 | (109.43 ms | 37431 tok/s)\n",
      "step  800/1000 | train loss 0.084985 | norm 0.1290 | lr 2.90e-06 | (108.05 ms | 37907 tok/s)\n",
      "step  801/1000 | train loss 0.084976 | norm 0.1601 | lr 2.87e-06 | (111.47 ms | 36746 tok/s)\n",
      "step  802/1000 | train loss 0.084966 | norm 0.1677 | lr 2.84e-06 | (107.95 ms | 37943 tok/s)\n",
      "step  803/1000 | train loss 0.084955 | norm 0.1553 | lr 2.81e-06 | (107.91 ms | 37958 tok/s)\n",
      "step  804/1000 | train loss 0.084944 | norm 0.1469 | lr 2.78e-06 | (108.37 ms | 37796 tok/s)\n",
      "step  805/1000 | train loss 0.084933 | norm 0.1480 | lr 2.76e-06 | (108.80 ms | 37646 tok/s)\n",
      "step  806/1000 | train loss 0.084923 | norm 0.1479 | lr 2.73e-06 | (108.93 ms | 37603 tok/s)\n",
      "step  807/1000 | train loss 0.084913 | norm 0.1499 | lr 2.70e-06 | (108.46 ms | 37766 tok/s)\n",
      "step  808/1000 | train loss 0.084903 | norm 0.1570 | lr 2.68e-06 | (107.59 ms | 38071 tok/s)\n",
      "step  809/1000 | train loss 0.084893 | norm 0.1548 | lr 2.65e-06 | (111.98 ms | 36578 tok/s)\n",
      "step  810/1000 | train loss 0.084881 | norm 0.1278 | lr 2.62e-06 | (110.84 ms | 36955 tok/s)\n",
      "step  811/1000 | train loss 0.084869 | norm 0.1041 | lr 2.60e-06 | (111.65 ms | 36686 tok/s)\n",
      "step  812/1000 | train loss 0.084862 | norm 0.1479 | lr 2.57e-06 | (109.12 ms | 37537 tok/s)\n",
      "step  813/1000 | train loss 0.084855 | norm 0.1835 | lr 2.54e-06 | (108.18 ms | 37861 tok/s)\n",
      "step  814/1000 | train loss 0.084847 | norm 0.2147 | lr 2.52e-06 | (107.38 ms | 38145 tok/s)\n",
      "step  815/1000 | train loss 0.084842 | norm 0.2545 | lr 2.49e-06 | (107.35 ms | 38157 tok/s)\n",
      "step  816/1000 | train loss 0.084833 | norm 0.2485 | lr 2.47e-06 | (109.36 ms | 37454 tok/s)\n",
      "step  817/1000 | train loss 0.084816 | norm 0.1673 | lr 2.44e-06 | (112.08 ms | 36545 tok/s)\n",
      "step  818/1000 | train loss 0.084805 | norm 0.1283 | lr 2.41e-06 | (108.92 ms | 37605 tok/s)\n",
      "step  819/1000 | train loss 0.084800 | norm 0.1884 | lr 2.39e-06 | (110.93 ms | 36925 tok/s)\n",
      "step  820/1000 | train loss 0.084790 | norm 0.1876 | lr 2.36e-06 | (108.46 ms | 37766 tok/s)\n",
      "step  821/1000 | train loss 0.084778 | norm 0.1289 | lr 2.34e-06 | (108.35 ms | 37804 tok/s)\n",
      "step  822/1000 | train loss 0.084769 | norm 0.1318 | lr 2.31e-06 | (108.18 ms | 37862 tok/s)\n",
      "step  823/1000 | train loss 0.084761 | norm 0.1516 | lr 2.29e-06 | (110.15 ms | 37187 tok/s)\n",
      "step  824/1000 | train loss 0.084752 | norm 0.1374 | lr 2.26e-06 | (110.54 ms | 37056 tok/s)\n",
      "step  825/1000 | train loss 0.084743 | norm 0.1235 | lr 2.24e-06 | (109.66 ms | 37352 tok/s)\n",
      "step  826/1000 | train loss 0.084733 | norm 0.1098 | lr 2.21e-06 | (108.64 ms | 37701 tok/s)\n",
      "step  827/1000 | train loss 0.084725 | norm 0.1215 | lr 2.19e-06 | (111.77 ms | 36647 tok/s)\n",
      "step  828/1000 | train loss 0.084718 | norm 0.1277 | lr 2.16e-06 | (113.10 ms | 36216 tok/s)\n",
      "step  829/1000 | train loss 0.084708 | norm 0.0995 | lr 2.14e-06 | (109.21 ms | 37505 tok/s)\n",
      "step  830/1000 | train loss 0.084699 | norm 0.0965 | lr 2.12e-06 | (108.59 ms | 37722 tok/s)\n",
      "step  831/1000 | train loss 0.084692 | norm 0.1135 | lr 2.09e-06 | (113.79 ms | 35996 tok/s)\n",
      "step  832/1000 | train loss 0.084684 | norm 0.1078 | lr 2.07e-06 | (109.73 ms | 37328 tok/s)\n",
      "step  833/1000 | train loss 0.084675 | norm 0.0955 | lr 2.04e-06 | (109.61 ms | 37369 tok/s)\n",
      "step  834/1000 | train loss 0.084667 | norm 0.0928 | lr 2.02e-06 | (110.26 ms | 37148 tok/s)\n",
      "step  835/1000 | train loss 0.084660 | norm 0.0923 | lr 2.00e-06 | (111.32 ms | 36794 tok/s)\n",
      "step  836/1000 | train loss 0.084652 | norm 0.0964 | lr 1.97e-06 | (109.73 ms | 37327 tok/s)\n",
      "step  837/1000 | train loss 0.084644 | norm 0.0968 | lr 1.95e-06 | (110.06 ms | 37216 tok/s)\n",
      "step  838/1000 | train loss 0.084637 | norm 0.0935 | lr 1.93e-06 | (108.52 ms | 37746 tok/s)\n",
      "step  839/1000 | train loss 0.084629 | norm 0.0893 | lr 1.90e-06 | (108.12 ms | 37885 tok/s)\n",
      "step  840/1000 | train loss 0.084622 | norm 0.1000 | lr 1.88e-06 | (109.28 ms | 37482 tok/s)\n",
      "step  841/1000 | train loss 0.084616 | norm 0.1214 | lr 1.86e-06 | (110.48 ms | 37074 tok/s)\n",
      "step  842/1000 | train loss 0.084611 | norm 0.1528 | lr 1.84e-06 | (109.50 ms | 37406 tok/s)\n",
      "step  843/1000 | train loss 0.084606 | norm 0.1871 | lr 1.81e-06 | (111.40 ms | 36769 tok/s)\n",
      "step  844/1000 | train loss 0.084599 | norm 0.1854 | lr 1.79e-06 | (136.18 ms | 30077 tok/s)\n",
      "step  845/1000 | train loss 0.084588 | norm 0.1295 | lr 1.77e-06 | (109.30 ms | 37475 tok/s)\n",
      "step  846/1000 | train loss 0.084580 | norm 0.0947 | lr 1.75e-06 | (107.45 ms | 38121 tok/s)\n",
      "step  847/1000 | train loss 0.084575 | norm 0.1383 | lr 1.72e-06 | (107.45 ms | 38118 tok/s)\n",
      "step  848/1000 | train loss 0.084569 | norm 0.1469 | lr 1.70e-06 | (108.29 ms | 37825 tok/s)\n",
      "step  849/1000 | train loss 0.084560 | norm 0.1018 | lr 1.68e-06 | (108.99 ms | 37581 tok/s)\n",
      "step  850/1000 | train loss 0.084553 | norm 0.0943 | lr 1.66e-06 | (107.38 ms | 38146 tok/s)\n",
      "step  851/1000 | train loss 0.084548 | norm 0.1270 | lr 1.64e-06 | (107.99 ms | 37928 tok/s)\n",
      "step  852/1000 | train loss 0.084541 | norm 0.1071 | lr 1.62e-06 | (110.06 ms | 37216 tok/s)\n",
      "step  853/1000 | train loss 0.084533 | norm 0.0851 | lr 1.60e-06 | (108.71 ms | 37677 tok/s)\n",
      "step  854/1000 | train loss 0.084528 | norm 0.1053 | lr 1.57e-06 | (107.81 ms | 37994 tok/s)\n",
      "step  855/1000 | train loss 0.084522 | norm 0.1024 | lr 1.55e-06 | (106.51 ms | 38457 tok/s)\n",
      "step  856/1000 | train loss 0.084515 | norm 0.0855 | lr 1.53e-06 | (108.27 ms | 37832 tok/s)\n",
      "step  857/1000 | train loss 0.084509 | norm 0.0873 | lr 1.51e-06 | (105.97 ms | 38653 tok/s)\n",
      "step  858/1000 | train loss 0.084503 | norm 0.0948 | lr 1.49e-06 | (107.18 ms | 38215 tok/s)\n",
      "step  859/1000 | train loss 0.084497 | norm 0.0885 | lr 1.47e-06 | (120.77 ms | 33917 tok/s)\n",
      "step  860/1000 | train loss 0.084491 | norm 0.0792 | lr 1.45e-06 | (107.76 ms | 38010 tok/s)\n",
      "step  861/1000 | train loss 0.084486 | norm 0.0867 | lr 1.43e-06 | (106.63 ms | 38413 tok/s)\n",
      "step  862/1000 | train loss 0.084480 | norm 0.0881 | lr 1.41e-06 | (107.78 ms | 38004 tok/s)\n",
      "step  863/1000 | train loss 0.084474 | norm 0.0762 | lr 1.39e-06 | (106.20 ms | 38570 tok/s)\n",
      "step  864/1000 | train loss 0.084468 | norm 0.0794 | lr 1.37e-06 | (107.64 ms | 38054 tok/s)\n",
      "step  865/1000 | train loss 0.084463 | norm 0.0828 | lr 1.35e-06 | (106.92 ms | 38310 tok/s)\n",
      "step  866/1000 | train loss 0.084457 | norm 0.0776 | lr 1.33e-06 | (108.50 ms | 37750 tok/s)\n",
      "step  867/1000 | train loss 0.084452 | norm 0.0762 | lr 1.31e-06 | (106.23 ms | 38558 tok/s)\n",
      "step  868/1000 | train loss 0.084447 | norm 0.0789 | lr 1.29e-06 | (107.73 ms | 38021 tok/s)\n",
      "step  869/1000 | train loss 0.084442 | norm 0.0815 | lr 1.27e-06 | (109.90 ms | 37272 tok/s)\n",
      "step  870/1000 | train loss 0.084437 | norm 0.0834 | lr 1.26e-06 | (108.57 ms | 37728 tok/s)\n",
      "step  871/1000 | train loss 0.084432 | norm 0.0920 | lr 1.24e-06 | (109.53 ms | 37396 tok/s)\n",
      "step  872/1000 | train loss 0.084428 | norm 0.1065 | lr 1.22e-06 | (108.64 ms | 37704 tok/s)\n",
      "step  873/1000 | train loss 0.084423 | norm 0.1169 | lr 1.20e-06 | (108.70 ms | 37683 tok/s)\n",
      "step  874/1000 | train loss 0.084418 | norm 0.1119 | lr 1.18e-06 | (109.12 ms | 37538 tok/s)\n",
      "step  875/1000 | train loss 0.084412 | norm 0.0944 | lr 1.16e-06 | (113.45 ms | 36104 tok/s)\n",
      "step  876/1000 | train loss 0.084408 | norm 0.0890 | lr 1.14e-06 | (108.70 ms | 37681 tok/s)\n",
      "step  877/1000 | train loss 0.084403 | norm 0.1006 | lr 1.13e-06 | (108.38 ms | 37792 tok/s)\n",
      "step  878/1000 | train loss 0.084399 | norm 0.0995 | lr 1.11e-06 | (108.28 ms | 37827 tok/s)\n",
      "step  879/1000 | train loss 0.084394 | norm 0.0866 | lr 1.09e-06 | (110.20 ms | 37170 tok/s)\n",
      "step  880/1000 | train loss 0.084389 | norm 0.0809 | lr 1.07e-06 | (107.89 ms | 37964 tok/s)\n",
      "step  881/1000 | train loss 0.084385 | norm 0.0861 | lr 1.06e-06 | (108.09 ms | 37894 tok/s)\n",
      "step  882/1000 | train loss 0.084381 | norm 0.0849 | lr 1.04e-06 | (107.00 ms | 38279 tok/s)\n",
      "step  883/1000 | train loss 0.084376 | norm 0.0791 | lr 1.02e-06 | (106.86 ms | 38329 tok/s)\n",
      "step  884/1000 | train loss 0.084372 | norm 0.0818 | lr 1.00e-06 | (109.79 ms | 37309 tok/s)\n",
      "step  885/1000 | train loss 0.084368 | norm 0.0804 | lr 9.88e-07 | (109.08 ms | 37551 tok/s)\n",
      "step  886/1000 | train loss 0.084364 | norm 0.0787 | lr 9.71e-07 | (109.71 ms | 37334 tok/s)\n",
      "step  887/1000 | train loss 0.084360 | norm 0.0775 | lr 9.55e-07 | (106.98 ms | 38288 tok/s)\n",
      "step  888/1000 | train loss 0.084356 | norm 0.0757 | lr 9.38e-07 | (107.25 ms | 38192 tok/s)\n",
      "step  889/1000 | train loss 0.084352 | norm 0.0747 | lr 9.22e-07 | (109.26 ms | 37490 tok/s)\n",
      "step  890/1000 | train loss 0.084348 | norm 0.0746 | lr 9.06e-07 | (108.91 ms | 37609 tok/s)\n",
      "step  891/1000 | train loss 0.084344 | norm 0.0738 | lr 8.90e-07 | (107.45 ms | 38121 tok/s)\n",
      "step  892/1000 | train loss 0.084340 | norm 0.0733 | lr 8.74e-07 | (107.84 ms | 37983 tok/s)\n",
      "step  893/1000 | train loss 0.084337 | norm 0.0737 | lr 8.58e-07 | (107.95 ms | 37942 tok/s)\n",
      "step  894/1000 | train loss 0.084333 | norm 0.0722 | lr 8.42e-07 | (108.43 ms | 37775 tok/s)\n",
      "step  895/1000 | train loss 0.084330 | norm 0.0726 | lr 8.27e-07 | (107.46 ms | 38117 tok/s)\n",
      "step  896/1000 | train loss 0.084326 | norm 0.0728 | lr 8.12e-07 | (108.83 ms | 37638 tok/s)\n",
      "step  897/1000 | train loss 0.084323 | norm 0.0714 | lr 7.96e-07 | (107.76 ms | 38010 tok/s)\n",
      "step  898/1000 | train loss 0.084319 | norm 0.0704 | lr 7.81e-07 | (109.04 ms | 37563 tok/s)\n",
      "step  899/1000 | train loss 0.084316 | norm 0.0718 | lr 7.66e-07 | (109.07 ms | 37553 tok/s)\n",
      "step  900/1000 | train loss 0.084313 | norm 0.0712 | lr 7.52e-07 | (107.64 ms | 38052 tok/s)\n",
      "step  901/1000 | train loss 0.084309 | norm 0.0696 | lr 7.37e-07 | (107.52 ms | 38096 tok/s)\n",
      "step  902/1000 | train loss 0.084306 | norm 0.0703 | lr 7.23e-07 | (107.55 ms | 38086 tok/s)\n",
      "step  903/1000 | train loss 0.084303 | norm 0.0709 | lr 7.08e-07 | (108.51 ms | 37749 tok/s)\n",
      "step  904/1000 | train loss 0.084300 | norm 0.0697 | lr 6.94e-07 | (109.87 ms | 37282 tok/s)\n",
      "step  905/1000 | train loss 0.084297 | norm 0.0698 | lr 6.80e-07 | (107.16 ms | 38222 tok/s)\n",
      "step  906/1000 | train loss 0.084294 | norm 0.0707 | lr 6.66e-07 | (107.92 ms | 37955 tok/s)\n",
      "step  907/1000 | train loss 0.084291 | norm 0.0700 | lr 6.52e-07 | (109.08 ms | 37550 tok/s)\n",
      "step  908/1000 | train loss 0.084288 | norm 0.0704 | lr 6.39e-07 | (111.71 ms | 36668 tok/s)\n",
      "step  909/1000 | train loss 0.084285 | norm 0.0717 | lr 6.25e-07 | (109.33 ms | 37465 tok/s)\n",
      "step  910/1000 | train loss 0.084283 | norm 0.0722 | lr 6.12e-07 | (108.76 ms | 37663 tok/s)\n",
      "step  911/1000 | train loss 0.084280 | norm 0.0735 | lr 5.99e-07 | (108.24 ms | 37843 tok/s)\n",
      "step  912/1000 | train loss 0.084277 | norm 0.0737 | lr 5.85e-07 | (107.54 ms | 38089 tok/s)\n",
      "step  913/1000 | train loss 0.084275 | norm 0.0736 | lr 5.73e-07 | (108.38 ms | 37793 tok/s)\n",
      "step  914/1000 | train loss 0.084272 | norm 0.0720 | lr 5.60e-07 | (108.73 ms | 37671 tok/s)\n",
      "step  915/1000 | train loss 0.084270 | norm 0.0699 | lr 5.47e-07 | (109.58 ms | 37378 tok/s)\n",
      "step  916/1000 | train loss 0.084267 | norm 0.0688 | lr 5.35e-07 | (108.62 ms | 37710 tok/s)\n",
      "step  917/1000 | train loss 0.084265 | norm 0.0690 | lr 5.22e-07 | (107.64 ms | 38051 tok/s)\n",
      "step  918/1000 | train loss 0.084262 | norm 0.0700 | lr 5.10e-07 | (111.21 ms | 36831 tok/s)\n",
      "step  919/1000 | train loss 0.084260 | norm 0.0704 | lr 4.98e-07 | (107.55 ms | 38086 tok/s)\n",
      "step  920/1000 | train loss 0.084258 | norm 0.0699 | lr 4.86e-07 | (107.83 ms | 37986 tok/s)\n",
      "step  921/1000 | train loss 0.084255 | norm 0.0688 | lr 4.74e-07 | (108.43 ms | 37776 tok/s)\n",
      "step  922/1000 | train loss 0.084253 | norm 0.0685 | lr 4.63e-07 | (109.30 ms | 37476 tok/s)\n",
      "step  923/1000 | train loss 0.084251 | norm 0.0688 | lr 4.51e-07 | (107.63 ms | 38055 tok/s)\n",
      "step  924/1000 | train loss 0.084249 | norm 0.0691 | lr 4.40e-07 | (109.57 ms | 37384 tok/s)\n",
      "step  925/1000 | train loss 0.084247 | norm 0.0691 | lr 4.28e-07 | (107.66 ms | 38045 tok/s)\n",
      "step  926/1000 | train loss 0.084245 | norm 0.0685 | lr 4.17e-07 | (108.67 ms | 37692 tok/s)\n",
      "step  927/1000 | train loss 0.084243 | norm 0.0682 | lr 4.06e-07 | (108.91 ms | 37608 tok/s)\n",
      "step  928/1000 | train loss 0.084241 | norm 0.0684 | lr 3.96e-07 | (107.86 ms | 37975 tok/s)\n",
      "step  929/1000 | train loss 0.084239 | norm 0.0685 | lr 3.85e-07 | (107.00 ms | 38282 tok/s)\n",
      "step  930/1000 | train loss 0.084237 | norm 0.0685 | lr 3.75e-07 | (107.66 ms | 38046 tok/s)\n",
      "step  931/1000 | train loss 0.084236 | norm 0.0682 | lr 3.64e-07 | (108.14 ms | 37875 tok/s)\n",
      "step  932/1000 | train loss 0.084234 | norm 0.0681 | lr 3.54e-07 | (108.12 ms | 37884 tok/s)\n",
      "step  933/1000 | train loss 0.084232 | norm 0.0682 | lr 3.44e-07 | (107.80 ms | 37998 tok/s)\n",
      "step  934/1000 | train loss 0.084231 | norm 0.0682 | lr 3.34e-07 | (108.22 ms | 37849 tok/s)\n",
      "step  935/1000 | train loss 0.084229 | norm 0.0681 | lr 3.24e-07 | (109.07 ms | 37554 tok/s)\n",
      "step  936/1000 | train loss 0.084227 | norm 0.0680 | lr 3.15e-07 | (107.67 ms | 38041 tok/s)\n",
      "step  937/1000 | train loss 0.084226 | norm 0.0681 | lr 3.05e-07 | (109.61 ms | 37369 tok/s)\n",
      "step  938/1000 | train loss 0.084224 | norm 0.0681 | lr 2.96e-07 | (107.69 ms | 38034 tok/s)\n",
      "step  939/1000 | train loss 0.084223 | norm 0.0680 | lr 2.87e-07 | (109.44 ms | 37426 tok/s)\n",
      "step  940/1000 | train loss 0.084221 | norm 0.0680 | lr 2.78e-07 | (108.75 ms | 37665 tok/s)\n",
      "step  941/1000 | train loss 0.084220 | norm 0.0680 | lr 2.69e-07 | (107.14 ms | 38232 tok/s)\n",
      "step  942/1000 | train loss 0.084219 | norm 0.0680 | lr 2.60e-07 | (108.25 ms | 37840 tok/s)\n",
      "step  943/1000 | train loss 0.084217 | norm 0.0679 | lr 2.51e-07 | (112.10 ms | 36539 tok/s)\n",
      "step  944/1000 | train loss 0.084216 | norm 0.0679 | lr 2.43e-07 | (109.60 ms | 37374 tok/s)\n",
      "step  945/1000 | train loss 0.084215 | norm 0.0679 | lr 2.35e-07 | (110.54 ms | 37054 tok/s)\n",
      "step  946/1000 | train loss 0.084214 | norm 0.0679 | lr 2.26e-07 | (109.38 ms | 37447 tok/s)\n",
      "step  947/1000 | train loss 0.084212 | norm 0.0679 | lr 2.18e-07 | (110.48 ms | 37074 tok/s)\n",
      "step  948/1000 | train loss 0.084211 | norm 0.0679 | lr 2.10e-07 | (108.31 ms | 37816 tok/s)\n",
      "step  949/1000 | train loss 0.084210 | norm 0.0679 | lr 2.03e-07 | (109.64 ms | 37358 tok/s)\n",
      "step  950/1000 | train loss 0.084209 | norm 0.0679 | lr 1.95e-07 | (108.13 ms | 37879 tok/s)\n",
      "step  951/1000 | train loss 0.084208 | norm 0.0679 | lr 1.88e-07 | (112.47 ms | 36419 tok/s)\n",
      "step  952/1000 | train loss 0.084207 | norm 0.0679 | lr 1.80e-07 | (107.97 ms | 37936 tok/s)\n",
      "step  953/1000 | train loss 0.084206 | norm 0.0679 | lr 1.73e-07 | (110.72 ms | 36994 tok/s)\n",
      "step  954/1000 | train loss 0.084205 | norm 0.0679 | lr 1.66e-07 | (109.60 ms | 37371 tok/s)\n",
      "step  955/1000 | train loss 0.084204 | norm 0.0679 | lr 1.59e-07 | (109.62 ms | 37366 tok/s)\n",
      "step  956/1000 | train loss 0.084204 | norm 0.0679 | lr 1.53e-07 | (109.85 ms | 37287 tok/s)\n",
      "step  957/1000 | train loss 0.084203 | norm 0.0679 | lr 1.46e-07 | (107.65 ms | 38048 tok/s)\n",
      "step  958/1000 | train loss 0.084202 | norm 0.0679 | lr 1.40e-07 | (107.87 ms | 37971 tok/s)\n",
      "step  959/1000 | train loss 0.084201 | norm 0.0679 | lr 1.33e-07 | (109.80 ms | 37304 tok/s)\n",
      "step  960/1000 | train loss 0.084200 | norm 0.0678 | lr 1.27e-07 | (110.03 ms | 37227 tok/s)\n",
      "step  961/1000 | train loss 0.084200 | norm 0.0678 | lr 1.21e-07 | (108.01 ms | 37922 tok/s)\n",
      "step  962/1000 | train loss 0.084199 | norm 0.0678 | lr 1.15e-07 | (109.22 ms | 37504 tok/s)\n",
      "step  963/1000 | train loss 0.084198 | norm 0.0678 | lr 1.10e-07 | (108.44 ms | 37772 tok/s)\n",
      "step  964/1000 | train loss 0.084198 | norm 0.0678 | lr 1.04e-07 | (107.02 ms | 38273 tok/s)\n",
      "step  965/1000 | train loss 0.084197 | norm 0.0678 | lr 9.88e-08 | (106.78 ms | 38360 tok/s)\n",
      "step  966/1000 | train loss 0.084197 | norm 0.0678 | lr 9.36e-08 | (107.75 ms | 38013 tok/s)\n",
      "step  967/1000 | train loss 0.084196 | norm 0.0678 | lr 8.85e-08 | (108.67 ms | 37693 tok/s)\n",
      "step  968/1000 | train loss 0.084196 | norm 0.0678 | lr 8.35e-08 | (107.44 ms | 38122 tok/s)\n",
      "step  969/1000 | train loss 0.084195 | norm 0.0678 | lr 7.87e-08 | (107.67 ms | 38042 tok/s)\n",
      "step  970/1000 | train loss 0.084195 | norm 0.0678 | lr 7.41e-08 | (109.23 ms | 37500 tok/s)\n",
      "step  971/1000 | train loss 0.084194 | norm 0.0678 | lr 6.96e-08 | (107.27 ms | 38184 tok/s)\n",
      "step  972/1000 | train loss 0.084194 | norm 0.0678 | lr 6.52e-08 | (107.82 ms | 37988 tok/s)\n",
      "step  973/1000 | train loss 0.084193 | norm 0.0678 | lr 6.10e-08 | (107.66 ms | 38047 tok/s)\n",
      "step  974/1000 | train loss 0.084193 | norm 0.0678 | lr 5.69e-08 | (107.17 ms | 38219 tok/s)\n",
      "step  975/1000 | train loss 0.084193 | norm 0.0678 | lr 5.30e-08 | (106.83 ms | 38343 tok/s)\n",
      "step  976/1000 | train loss 0.084192 | norm 0.0678 | lr 4.92e-08 | (106.98 ms | 38288 tok/s)\n",
      "step  977/1000 | train loss 0.084192 | norm 0.0678 | lr 4.56e-08 | (106.72 ms | 38380 tok/s)\n",
      "step  978/1000 | train loss 0.084192 | norm 0.0678 | lr 4.21e-08 | (107.40 ms | 38137 tok/s)\n",
      "step  979/1000 | train loss 0.084192 | norm 0.0678 | lr 3.88e-08 | (107.63 ms | 38057 tok/s)\n",
      "step  980/1000 | train loss 0.084191 | norm 0.0678 | lr 3.56e-08 | (107.92 ms | 37954 tok/s)\n",
      "step  981/1000 | train loss 0.084191 | norm 0.0678 | lr 3.26e-08 | (110.49 ms | 37070 tok/s)\n",
      "step  982/1000 | train loss 0.084191 | norm 0.0678 | lr 2.97e-08 | (109.74 ms | 37326 tok/s)\n",
      "step  983/1000 | train loss 0.084191 | norm 0.0678 | lr 2.70e-08 | (109.10 ms | 37545 tok/s)\n",
      "step  984/1000 | train loss 0.084191 | norm 0.0678 | lr 2.44e-08 | (106.86 ms | 38329 tok/s)\n",
      "step  985/1000 | train loss 0.084190 | norm 0.0678 | lr 2.19e-08 | (108.97 ms | 37587 tok/s)\n",
      "step  986/1000 | train loss 0.084190 | norm 0.0678 | lr 1.97e-08 | (107.78 ms | 38004 tok/s)\n",
      "step  987/1000 | train loss 0.084190 | norm 0.0678 | lr 1.75e-08 | (108.60 ms | 37717 tok/s)\n",
      "step  988/1000 | train loss 0.084190 | norm 0.0678 | lr 1.55e-08 | (105.73 ms | 38741 tok/s)\n",
      "step  989/1000 | train loss 0.084190 | norm 0.0678 | lr 1.37e-08 | (109.01 ms | 37574 tok/s)\n",
      "step  990/1000 | train loss 0.084190 | norm 0.0678 | lr 1.20e-08 | (106.87 ms | 38326 tok/s)\n",
      "step  991/1000 | train loss 0.084190 | norm 0.0678 | lr 1.04e-08 | (109.24 ms | 37496 tok/s)\n",
      "step  992/1000 | train loss 0.084190 | norm 0.0678 | lr 8.99e-09 | (109.20 ms | 37509 tok/s)\n",
      "step  993/1000 | train loss 0.084190 | norm 0.0678 | lr 7.74e-09 | (108.99 ms | 37582 tok/s)\n",
      "step  994/1000 | train loss 0.084190 | norm 0.0678 | lr 6.63e-09 | (107.46 ms | 38117 tok/s)\n",
      "step  995/1000 | train loss 0.084190 | norm 0.0678 | lr 5.66e-09 | (110.43 ms | 37092 tok/s)\n",
      "step  996/1000 | train loss 0.084190 | norm 0.0678 | lr 4.85e-09 | (108.67 ms | 37694 tok/s)\n",
      "step  997/1000 | train loss 0.084190 | norm 0.0678 | lr 4.18e-09 | (113.08 ms | 36223 tok/s)\n",
      "step  998/1000 | train loss 0.084190 | norm 0.0678 | lr 3.67e-09 | (110.62 ms | 37027 tok/s)\n",
      "step  999/1000 | train loss 0.084190 | norm 0.0678 | lr 3.30e-09 | (108.23 ms | 37844 tok/s)\n",
      "step 1000/1000 | train loss 0.084190 | norm 0.0678 | lr 3.07e-09 | (109.12 ms | 37536 tok/s)\n",
      "step 1001/1000 | train loss 0.084189 | norm 0.0678 | lr 3.00e-09 | (110.33 ms | 37123 tok/s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timings = []\n",
    "norm = -1.0   # Normalizasyon için varsayılan değer\n",
    "for step in range(num_iterations + 1):\n",
    "    t0 = time.time()  # Başlangıç zamanı\n",
    "\n",
    "    last_step = (step == num_iterations)\n",
    "\n",
    "    # Arada bir doğrulama veri setinde değerlendirme yapılır\n",
    "    if (val_loss_every > 0 \\\n",
    "        and (step % val_loss_every == 0 or last_step)) \\\n",
    "        and (val_loader is not None):\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for _ in range(val_max_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, y, return_logits=False)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= val_max_steps\n",
    "        # Konsola ve dosyaya kaydetme\n",
    "        print(f\"val loss {val_loss}\")\n",
    "\n",
    "    # Arada bir modelin örnekleme yapılması, yalnızca ana süreçte gerçekleşir\n",
    "    if (sample_every > 0 \\\n",
    "        and (step % sample_every == 0 or last_step)) \\\n",
    "        and master_process:\n",
    "        model.eval()\n",
    "        # Başlangıç için bir dizi oluşturulur, \"\" ile yeni bir dizinin başlangıcı belirtilir\n",
    "        start_ids = [63]\n",
    "        xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        max_new_tokens = 32\n",
    "        temperature = 1.0\n",
    "        top_k = 40\n",
    "        yg = ham_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print('---------------')\n",
    "        print(decode(yg[0].tolist()))\n",
    "        print('---------------')\n",
    "\n",
    "    # Modelin eğitim moduna geçmesi\n",
    "    model.train()\n",
    "\n",
    "    # Gradient birikimi yaparak istenen toplam veri boyutuna ulaşma\n",
    "    lossf = 0.0  # Gradient birikimi aşamasında ortalama kaybı almak için\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # Eğitim veri yükleyicisinden bir grup al\n",
    "        if not overfit_single_batch \\\n",
    "            or (overfit_single_batch and step == 0 and micro_step == 0):\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # İleri geçiş\n",
    "        with ctx:\n",
    "            _, loss = model(x, y, return_logits=False)\n",
    "            # Gradyan birikimini hesaba katmak için kaybı ölçeklendiriyoruz\n",
    "            loss = loss / grad_accum_steps\n",
    "            lossf += loss.detach()  # Ortalama kaybı takip etme\n",
    "        # Geriye doğru geçiş\n",
    "        if not inference_only:\n",
    "            loss.backward()\n",
    "\n",
    "    lossf = lossf.item()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # Bu iterasyon için öğrenme oranını belirleme ve ayarlama\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Optimizer'ı güncelleme\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Eğitim bölümünün sonu\n",
    "    # Buradan sonrası sadece tanısal amaçlı, yazdırma ve günlüklere ait\n",
    "    # CPU'da tüm cihaz işlemlerinin tamamlanmasını bekleyin, böylece altındaki doğru iterasyon süreleri alabiliriz\n",
    "    if device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Zamanı ve çıktıları yazdırma\n",
    "    t1 = time.time()\n",
    "    tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
    "    print(f\"step {step+1:4d}/{num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
    "\n",
    "    # En son 20 iterasyonun zamanlarını takip etme\n",
    "    if step > 0 and step > num_iterations - 20:\n",
    "        timings.append(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0785,  0.0645,  0.0373, -0.0839,  0.0397],\n",
       "        [-0.0535,  0.0411,  0.0644,  0.0563,  0.0212],\n",
       "        [ 0.0361, -0.0643, -0.0469, -0.0099, -0.0125],\n",
       "        [-0.0480,  0.0328, -0.0410,  0.0688,  0.0577],\n",
       "        [-0.0423, -0.0519,  0.0101, -0.0371, -0.0678]], device='mps:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Eğitim öncesi\n",
    "tensor([[ 0.0385,  0.0297,  0.0180, -0.0421,  0.0136],\n",
    "        [-0.0195,  0.0192,  0.0324,  0.0290,  0.0054],\n",
    "        [ 0.0195, -0.0203, -0.0108, -0.0088, -0.0063],\n",
    "        [-0.0099,  0.0227, -0.0092,  0.0284,  0.0170],\n",
    "        [-0.0161, -0.0224,  0.0039, -0.0156, -0.0358]], device='mps:0',\n",
    "       grad_fn=<SliceBackward0>)\n",
    "\n",
    "Varmak istediğimiz yer\n",
    "tensor([[ 0.0839,  0.0716,  0.0449, -0.0960,  0.0551],\n",
    "        [-0.0607,  0.0496,  0.0738,  0.0693,  0.0313],\n",
    "        [ 0.0485, -0.0739, -0.0587, -0.0393,  0.0301],\n",
    "        [-0.0583,  0.0514, -0.0589,  0.0752,  0.0701],\n",
    "        [-0.0523, -0.0578, -0.0065, -0.0553, -0.0861]], device='mps:0',\n",
    "       grad_fn=<SliceBackward0>)\n",
    " \"\"\"\n",
    "model.lm_head.weight[:5, :5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final 20 iters avg: 110.266ms\n",
      "peak memory consumption: 0 MiB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Son 20 zaman ölçümünün ortalamasını alarak yazdırma\n",
    "timings = timings[-20:]\n",
    "print(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "\n",
    "# Zirve bellek tüketimini yazdırma (CUDA için geçerlidir)\n",
    "print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Örnek giriş:  \n",
      "Üretilen çıktı:  yeni doğanlarımıza öğretiyoruz b\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "calistir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
